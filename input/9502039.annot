<?xml version='1.0' encoding='ISO-8859-1'?>
<PAPER>
<METADATA>
<FILENO>9502039</FILENO>
<TITLE> Multilingual Sentence Categorization according to Language </TITLE>
<AUTHORS>
<AUTHOR>Emmanuel Giguet</AUTHOR>
</AUTHORS>
<APPEARED><CONFERENCE TYPE='Workshop'>EACL</CONFERENCE><YEAR>1995</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Cl </CLASSIFICATION>
</METADATA>
<ABSTRACT>
<A-S ID='A-0' IA='BKG' AZ='BKG'> Issues in sentence categorization according to language is fundamental for NLP , especially in document processing . </A-S>
<A-S ID='A-1' IA='BKG' AZ='BKG'> In fact , with the growing amount of multilingual text corpus data becoming available , sentence categorization , leading to multilingual text structure , opens a wide range of applications in multilingual text analysis such as information retrieval or preprocessing of multilingual syntactic parser . </A-S>
<A-S ID='A-2' IA='BKG' AZ='BKG'> The major difficulties in sentence categorization are convergence and textual errors . </A-S>
<A-S ID='A-3' IA='BKG' AZ='BKG'> Convergence since dealing with short entries involve discarding languages from few clues . </A-S>
<A-S ID='A-4' IA='BKG' AZ='BKG'> Textual errors since documents coming from different electronic ways may contain spelling and grammatical errors as well as character recognition errors generated by OCR . </A-S>
<A-S ID='A-5' IA='OWN' AZ='AIM'> We describe here an approach to sentence categorization which has the originality to be based on natural properties of languages with no training set dependency . </A-S>
<A-S ID='A-6' IA='OWN' AZ='OWN'> The implementation is fast , small , robust and textual errors tolerant . </A-S>
<A-S ID='A-7' IA='OWN' AZ='OWN'> Tested for french , english , spanish and german discrimination , the system gives very interesting results , achieving in one test 99.4 % correct assignments on real sentences . </A-S>
<A-S ID='A-8' IA='OWN' AZ='OWN'> The resolution power is based on grammatical words ( not the most common words ) and alphabet . </A-S>
<A-S ID='A-9' IA='OWN' AZ='OWN'> Having the grammatical words and the alphabet of each language at its disposal , the system computes for each of them its likelihood to be selected . </A-S>
<A-S ID='A-10' IA='OWN' AZ='OWN'> The name of the language having the optimum likelihood will tag the sentence -- but non resolved ambiguities will be maintained . </A-S>
<A-S ID='A-11' IA='OWN' AZ='OWN'> We will discuss the reasons which lead us to use these linguistic facts and present several directions to improve the system 's classification performance . </A-S>
<A-S ID='A-12' IA='OWN' AZ='OWN'> Categorization sentences with linguistic properties shows that difficult problems have sometimes simple solutions . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Categorization according to Language </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-1'> From Text Categorization ... </HEADER>
<P>
<S ID='S-0' IA='BKG' AZ='BKG'> Emergence of text categorization according to language came with the need of processing texts coming from all over the world . </S>
<S ID='S-1' IA='BKG' AZ='BKG' R='BKG' HUMAN='BACKG'> The goal of text categorization is to tag texts with the name of the language in which they are written . </S>
<S ID='S-2' IA='BKG' AZ='BKG'> Information retrieval is the main application field . </S>
</P>
<P>
<S ID='S-3' IA='OTH' AZ='OTH' R='OTH' HUMAN='RWRK_prev'> To do this job , the traditionnal way is to exploit the difference between letter combinations in different languages <REF TYPE='P'>Cavnar and Trenkle 1994</REF> . </S>
<S ID='S-4' IA='OTH' AZ='OTH'> For each language , the system computes from a training set a profile based on frequency ( or probability ) of letter sequences . </S>
<S ID='S-5' IA='OTH' AZ='OTH'> Then , for a given text , it computes a profile and select the language which has the closer profile . </S>
</P>
<P>
<S ID='S-6' IA='OTH' AZ='CTR' R='CTR' HUMAN='PUPR_weak'> While some text categorization systems give very good results , the major problem is that their quality is entirely based on the training set . </S>
<S ID='S-7' IA='OTH' AZ='CTR' R='CTR'> Profiles require a lot of data to converge and building a large representative training set is a real problem . </S>
<S ID='S-8' IA='OTH' AZ='CTR' R='CTR'> Moreover , this method assume that texts are monolingual and results will be affected when dealing with multilingual texts . </S>
<S ID='S-9' IA='OTH' AZ='CTR'> It does not care about natural language properties : it only considers texts as streams of characters . </S>
<S ID='S-10' IA='OTH' AZ='CTR'> There is no linguistic justification . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-2'> ... to Multilingual Sentence Categorization </HEADER>
<P>
<S ID='S-11' IA='BKG' AZ='BKG'> Today , the problem is quiet different . </S>
<S ID='S-12' IA='BKG' AZ='BKG'> Texts are more and more multilingual ( especially due to citations ) and we don't have enough tools to process them efficiently . </S>
<S ID='S-13' IA='BKG' AZ='BKG'> Tagging sentences with the name of their language solves this problem by switching each application in function of the language . </S>
<S ID='S-14' IA='BKG' AZ='BKG'> This affects the whole NLP , Information retrieval is not the only field to be concerned : syntactic analysis and every applications based on it are concerned , making study about one particular language in multilingual texts without parasitic noise is also possible . </S>
</P>
<P>
<S ID='S-15' IA='BKG' AZ='BKG'> Using the previous method is not possible because the sentence is a too small unit to converge . </S>
<S ID='S-16' IA='BKG' AZ='BKG'> The analysis method must be more precise to reveal each possible change of language . </S>
</P>
<P>
<S ID='S-17' IA='BKG' AZ='BKG'> We remark that a change of language in a text could appear at each change of sentence ( more often paragraph ) or in each included segment via quotes , parenthesis , dashes or colons . </S>
<S ID='S-18' IA='BKG' AZ='OWN'> We will call sentence the traditionnal sentence but also each segment included in it . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-3'> Multilingual Sentence Categorization </HEADER>
<P>
<S ID='S-19' IA='OWN' AZ='AIM' R='AIM'> Studying quantities of texts , we try to understand as well as possible ways to discriminate languages . </S>
<S ID='S-20' IA='OWN' AZ='TXT' START='Y'> We present in this section the results of our research which has been implemented and in the next section , other directions which seems obviously promising . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-4'> Grammatical Words as Discriminant </HEADER>
<P>
<S ID='S-21' IA='OWN' AZ='TXT' R='OWN' HUMAN='SOLU_local'> In this section , we are going to motivate the reasons which lead us to choose grammatical words as discriminant . </S>
</P>
<P>
<S ID='S-22' IA='OWN' AZ='OWN'> Grammatical words are proper to each language and are in a whole different from one language to another . </S>
<S ID='S-23' IA='OWN' AZ='OWN'> Moreover , they are short , not numerous and we can easily build an exhaustive list . </S>
<S ID='S-24' IA='OWN' AZ='OWN'> So , these words can be use as discriminant of language . </S>
<S ID='S-25' IA='OWN' AZ='OWN'> But can we use them as discriminant of sentences . </S>
</P>
<P>
<S ID='S-26' IA='OWN' AZ='OWN'> Grammatical words in sentences represent on average about 50 % of words . </S>
<S ID='S-27' IA='OWN' AZ='OWN'> They can't be omitted because they structure sentences and make them understandable . </S>
<S ID='S-28' IA='OWN' AZ='OWN'> Furthermore , relying on grammatical words allows textual errors tolerance and foreign words import from other languages ( usual in scientific texts ) . </S>
<S ID='S-29' IA='OWN' AZ='OWN'> It 's also important to note that foreign words import concerns nouns , verbs , adjectives but never grammatical words . </S>
</P>
<P>
<S ID='S-30' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_local;PUPR_local'> These rules will allow us to categorize sentences which have enough grammatical words but in short sentences ( less than 10 words ) , there are few grammatical words , and by the way , few clues . </S>
<S ID='S-31' IA='OWN' AZ='OWN'> We must introduce new knowledges to improve short sentences categorization . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-5'> Using the Alphabet </HEADER>
<P>
<S ID='S-32' IA='OWN' AZ='OWN'> To improve categorization of short sentences , a simple way is the use of the alphabet . </S>
<S ID='S-33' IA='OWN' AZ='OWN'> Alphabets are proper to each language and even if they have a great common part , some signs such as accents allows discrimination between them . </S>
<S ID='S-34' IA='OWN' AZ='OWN'> This is not the only way to improve categorization and we will see in section <CREF/> other possible issues . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-6'> Notes </HEADER>
<P>
<S ID='S-35' IA='OWN' AZ='OWN'> It is interesting that , using these knowledges , this system will be coherent with multilingual syntactic parsers which only rely on grammatical words and endings . </S>
<S ID='S-36' IA='OWN' AZ='OWN'> So , the categorization system can constitute a switch for these parsers <REF  TYPE='P'>Vergne 1993</REF>, <REF  TYPE='P'>Vergne 1994</REF> . </S>
</P>
<P>
<S ID='S-37' IA='OWN' AZ='OWN'> We can also remark that using grammatical words is different from using most common words . </S>
<S ID='S-38' IA='OWN' AZ='OWN'> In fact , most common words require training set dependency and it is well known that a representative training set is very difficult to get . </S>
<S ID='S-39' IA='OWN' AZ='OWN'> The number of words to hold is quiet subjective . </S>
<S ID='S-40' IA='OWN' AZ='OWN'> Moreover , frequency is relative to texts , not to sentences . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-7'> Improving Categorization </HEADER>
<P>
<S ID='S-41' IA='OWN' AZ='OWN'> There are two levels to improve sentences categorization : a level below using words morphology and a level above using text structure . </S>
<S ID='S-42' IA='OWN' AZ='OWN'> These improvements haven't been implemented yet and will be the object of further works . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-8'> Knowledge upon Words Morphology </HEADER>
<P>
<S ID='S-43' IA='OWN' AZ='OWN'> Mainly two ways can be explore to improve categorization , using natural languages properties : </S>
</P>
<P>
<S ID='S-44' IA='OWN' AZ='OWN'> Syllabation : </S>
<S ID='S-45' IA='OWN' AZ='OWN'> the idea is to check the good syllabation of words in a language . </S>
<S ID='S-46' IA='OWN' AZ='OWN'> It requires to distinguish first , middles and last syllabs . </S>
<S ID='S-47' IA='OWN' AZ='OWN'> ( Using only endings seems to be a possible way ) </S>
</P>
<P>
<S ID='S-48' IA='OWN' AZ='OWN'> Sequences of voyells or consonants : </S>
<S ID='S-49' IA='OWN' AZ='OWN'> the idea is that these sequences are proper to each language . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-9'> Using Text Structure </HEADER>
<P>
<S ID='S-50' IA='OWN' AZ='OWN'> When dealing with texts , we can also use heuristical knowledge about text structure : </S>
</P>
<P>
<S ID='S-51' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> In a same paragraph , contiguous sentences are written in the same language </S>
<S ID='S-52' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> Titles of a paragraph are written in the same language as their body </S>
<S ID='S-53' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> Included blocks in a sentence ( via parenthesis , ... ) are written in the same language as the sentence . </S>
</P>
<P>
<S ID='S-54' IA='OWN' AZ='OWN'> An interesting tool to build is a general document structure recognizer . </S>
<S ID='S-55' IA='OWN' AZ='CTR' R='CTR'> Theoritical issues in this field are in progress <REF TYPE='P'>Lucas et al. 1993</REF> , <REF TYPE='P'>Lucas 1992</REF> but as far as we know no implementation has been done yet . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-10'> Implementation </HEADER>
<P>
<S ID='S-56' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_prop'> The implementation of this research can be divided in two parts : sentence tokenization and language classification . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-11'> Sentence Tokenization </HEADER>
<P>
<S ID='S-57' IA='OWN' AZ='OWN'> Sentence tokenization is a problem in itsef because documents may come through different electronic ways . </S>
<S ID='S-58' IA='OWN' AZ='OWN'> Also a sentence doesn't always start with a capitalized letter and finish with a full stop ( especially in emails ) . </S>
<S ID='S-59' IA='OWN' AZ='OWN'> Texts are not formated and miscellaneous characters can be found everywhere . </S>
</P>
<P>
<S ID='S-60' IA='OWN' AZ='OWN'> Acronyms , abbreviations , full names and numbers increase the problem by inserting points and / or spaces everywhere without following any rule . </S>
<S ID='S-61' IA='OWN' AZ='OWN'> But , no rule can ever exist in free style texts . </S>
</P>
<P>
<S ID='S-62' IA='OWN' AZ='OWN'> We wrote a robust sentence parser which solves the majority of these cases , allowing us to categorize in good conditions multilingual sentences . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-12'> Language Classification </HEADER>
<P>
<S ID='S-63' IA='OWN' AZ='OWN'> The realization simply implements the previous ideas . </S>
</P>
<P>
<S ID='S-64' IA='OWN' AZ='OWN'> To manage the possible points of change of language via included segments ( see section <CREF/> ) , the language classification procedure uses a recursive algorithm to easily handle changes of context . </S>
</P>
<P>
<S ID='S-65' IA='OWN' AZ='OWN'> The classification principle is the following : </S>
</P>
<P>
<S ID='S-66' IA='OWN' AZ='OWN'> For each word of the sentence : </S>
<S ID='S-67' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> Checked whether the word belongs to the grammatical words list of some languages . </S>
<S ID='S-68' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> If so , incremented their likelihood to be selected . </S>
<S ID='S-69' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> Checked whether the word morphology lets think it belongs to some languages . </S>
<S ID='S-70' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> If so , incremented their likelihood to be selected . </S>
</P>
<P>
<S ID='S-71' IA='OWN' AZ='OWN'> Tag the sentence with the names of the languages which have the same and highest likelihood . </S>
</P>
<P>
<S ID='S-72' IA='OWN' AZ='OWN'> This algorithm has a linear complexity in time . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-13'> Evaluation </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-14'> The Test-Bed </HEADER>
<P>
<S ID='S-73' IA='OWN' AZ='OWN'> The test-bed set has been prepared to process French , English , Spanish and German . </S>
<S ID='S-74' IA='OWN' AZ='OWN'> We use dictionnaries to get the grammatical words of each language ( see table <CREF/> ) and their alphabet . </S>
</P>
<P>
<S ID='S-75' IA='OWN' AZ='OWN'> We decided to use different kinds of documents to test robustness , speed , precision and textual errors tolerance . </S>
<S ID='S-76' IA='OWN' AZ='OWN'> So , we collected scientific texts , emails and novels ( see table <CREF/> ) . </S>
<IMAGE ID='I-0'/>
<IMAGE ID='I-1'/>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-15'> Results </HEADER>
<P>
<S ID='S-77' IA='OWN' AZ='OWN'> The results we obtained were expected . </S>
<S ID='S-78' IA='OWN' AZ='OWN'> They express the fact that a sentence is usually written with grammatical words and that grammatical words are totally discriminant for sentences of more than 8 words . </S>
</P>
<P>
<S ID='S-79' IA='OWN' AZ='OWN'> From 1 to 3 words , there are mainly total undeterminations . </S>
<S ID='S-80' IA='OWN' AZ='OWN'> In fact , the corpus shows that we are processing included segments ( via quotes and parenthesis ) and there are no grammatical words and few clues to rely on . </S>
<S ID='S-81' IA='OWN' AZ='OWN'> Deductions really start between 4 and 6 words . </S>
<S ID='S-82' IA='OWN' AZ='OWN'> Here , sentences and grammatical words appear but in few quantities to allow a perfect deduction . </S>
</P>
<P>
<S ID='S-83' IA='OWN' AZ='OWN' R='OWN' HUMAN='CLCO'> These results show that alphabets are not good enough to discriminate short sentences . </S>
<S ID='S-84' IA='OWN' AZ='OWN'> Methods described in <CREF/> must be implemented to improve results in this case . </S>
<IMAGE ID='I-2'/>
</P>
<P>
<S ID='S-85' IA='OWN' AZ='OWN'> In table <CREF/> , with the french corpus , the program always succeeds in isolating a single language for all the sentences containing from 8 to 125 words . </S>
<S ID='S-86' IA='OWN' AZ='OWN'> For less than 8 words there are still ambiguities or total undetermination . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-16'> Errors </HEADER>
<P>
<S ID='S-87' IA='OWN' AZ='OWN'> Isolating a single language does not mean exactly isolating the right language . </S>
<S ID='S-88' IA='OWN' AZ='OWN'> The error rate is about 0.01 % and concerns very short sentences ( e mail where e is analysed as Spanish ) , a change of language without quotes in a sentence or an unexpected language ( the Latin Orbi et Urbi ) . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-17'> Conclusion </HEADER>
<P>
<S ID='S-89' IA='OWN' AZ='AIM' R='AIM' HUMAN='SOLU_prop' START='Y'> This classification method is based on texts observation and understanding of their natural properties . </S>
<S ID='S-90' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_prop'> It does not depend on training sets and converges fast enough to achieve very good results on sentences . </S>
</P>
<P>
<S ID='S-91' IA='OWN' AZ='OWN'> This tool is now a switch of Jacques Vergne 's multilingual syntactic parser ( for french , english and spanish ) . </S>
</P>
<P>
<S ID='S-92' IA='OWN' AZ='OWN' R='OWN' HUMAN='CLCO'> The aim of this paper is also to point that the more the linguistic properties of the object are used , the best the results are . </S>
</P>
</DIV>
</BODY>
<REFERENCES>
<REFERENCE>
William B. <SURNAME>Cavnar</SURNAME> and John M. <SURNAME>Trenkle</SURNAME>.
<DATE>1994</DATE>.
N-gram-based text categorization.
In Symposium On Document Analysis and Information Retrieval,
  pages 161-176, University of Nevada, Las Vegas.
</REFERENCE>
<REFERENCE>
Nadine <SURNAME>Lucas</SURNAME>, Nishina <SURNAME>Kikuko</SURNAME>, Akiba <SURNAME>Tomoyoshi</SURNAME>, and Surech K.<SURNAME>G</SURNAME>.
<DATE>1993</DATE>.
Discourse analysis of scientific textbooks in japanese : a tool for
  producing automatic summaries.
Technical Report 93TR-0004, Department of Computer Science, Tokyo
  Institute of Technology, Meguro-ku Ookayama 2-12-1, Tokyo 152, Japan, March.
</REFERENCE>
<REFERENCE>
Nadine <SURNAME>Lucas</SURNAME>.
<DATE>1992</DATE>.
Syntaxe du paragraphe dans les textes scientifiques en japonais et en
  français.
In Colloque international : Parcours linguistiques de discours
  spécialisé, Université Paris III, Septembre.
</REFERENCE>
<REFERENCE>
Jacques <SURNAME>Vergne</SURNAME>.
<DATE>1993</DATE>.
Syntactic properties of natural languages and application to
  automatic parsing.
In SEPLN 93 congress, Granada, Spain, August. Sociedad
  Espaola para el Procesamiento del Lenguaje Natural.
</REFERENCE>
<REFERENCE>
Jacques <SURNAME>Vergne</SURNAME>.
<DATE>1994</DATE>.
A non recursive sentence segmentation, applied to parsing of linear
  complexity in time.
In New Methods in Language Processing, pages 234-241, June.
</REFERENCE>
</REFERENCES>
</PAPER>
