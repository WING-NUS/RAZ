<?xml version='1.0' encoding='ISO-8859-1'?>
<PAPER>
<METADATA>
<FILENO>9502022</FILENO>
<TITLE> Stochastic HPSG </TITLE>
<AUTHORS>
<AUTHOR>Chris Brew</AUTHOR>
</AUTHORS>
<APPEARED><CONFERENCE>EACL</CONFERENCE><YEAR>1995</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Gr.Fm  Lg.Pr.St </CLASSIFICATION>
</METADATA>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-0' IA='OWN' AZ='AIM'> In this paper we provide a probabilistic interpretation for typed feature structures very similar to those used by <REFAUTHOR>Pollard and Sag</REFAUTHOR> . </A-S>
<A-S ID='A-1' IA='OWN' AZ='OWN'> We begin with a version of the interpretation which lacks a treatment of re-entrant feature structures , then provide an extended interpretation which allows them . </A-S>
<A-S ID='A-2' IA='OWN' AZ='OWN'> We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' ABSTRACTC='A-0' IA='OWN' AZ='AIM' R='AIM' HUMAN='TOPIC' START='Y'> The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures . </S>
<S ID='S-1' IA='OWN' AZ='BAS'> Our techniques apply to the feature structures described by <REF TYPE='A'>Carpenter 1992</REF> . </S>
<S ID='S-2' IA='OWN' AZ='BAS'> Since these structures are the ones which are used in by <REF TYPE='A'>Pollard and Sag 1994</REF> their relevance to computational grammars is apparent . </S>
<S ID='S-3' IA='OWN' AZ='BKG'> On the basis of the usefulness of probabilistic context-free grammars <REF TYPE='P'>Charniak 1993</REF> , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case . </S>
</P>
<P>
<S ID='S-4' IA='OWN' AZ='TXT'> The paper is structured as follows . </S>
<S ID='S-5' IA='OWN' AZ='TXT'> We start by reviewing the training and use of probabilistic context-free grammars ( PCFGs ) . </S>
<S ID='S-6' IA='OWN' AZ='TXT' R='OWN' HUMAN='PUPR'> We then develop a technique to allow analogous probabilistic annotations on type hierarchies . </S>
<S ID='S-7' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_prop'> This gives us a clear account of the relationship between a large class of feature structures and their probabilities , but does not treat re-entrancy . </S>
<S ID='S-8' IA='OWN' AZ='TXT' R='OWN' HUMAN='PUPR;SOLU_local'> We conclude by sketching a technique which does treat such structures . </S>
<S ID='S-9' IA='OWN' AZ='CTR' R='CTR' HUMAN='PUPR_new'> While we know of previous work which associates scores with feature structures <REF TYPE='P'>Kim 1994</REF> are not aware of any previous treatment which makes explicit the link to classical probability theory . </S>
</P>
<P>
<S ID='S-10' IA='OWN' AZ='BAS' R='BAS'> We take a slightly unconventional perspective on feature structures , because it is easier to cast our theory within the more general framework of incremental description refinement <REF TYPE='P'>Mellish 1988</REF> than to exploit the usual metaphors of constraint-based grammar . </S>
<S ID='S-11' IA='OWN' AZ='OWN'> In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings , because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Probabilistic interpretation of PCFGs </HEADER>
<P>
<S ID='S-12' IA='BKG' AZ='TXT'> We review the standard probabilistic interpretation of PCFGs . </S>
</P>
<P>
<S ID='S-13' IA='BKG' AZ='OTH'> A PCFG is a four-tuple <EQN/> , where W is a set of terminal symbols <EQN/> , N is a set of non-terminal symbols <EQN/> , <EQN/> is the starting symbol and R is a set of rules of the form <EQN/> , where <EQN/> is a string of terminals and non-terminals . </S>
<S ID='S-14' IA='BKG' AZ='OTH'> Each rule has a probability <EQN/> and the probabilities for all the rules that expand a given non-terminal must sum to one . </S>
<S ID='S-15' IA='BKG' AZ='OTH'> We associate probabilities with partial phrase markers , which are sets of terminal and non-terminal nodes generated by beginning from the starting node successively expanding non-terminal leaves of the partial tree . </S>
<S ID='S-16' IA='BKG' AZ='OTH'> Phrase markers are those partial phrase markers which have no non-terminal leaves . </S>
<S ID='S-17' IA='BKG' AZ='OTH'> Probabilities are assigned by the following inductive definition : </S>
</P>
<P>
<S ID='S-18' TYPE='ITEM' IA='BKG' AZ='OTH' TYPE='ITEM'> <EQN/>  </S>
<S ID='S-19' TYPE='ITEM' IA='BKG' AZ='OTH' TYPE='ITEM'> If T is a partial phrase marker , and T ' is a partial phrase marker which differs from it only in that a single non-terminal node <EQN/> in T has been expanded to <EQN/> in T ' , then <EQN/> . </S>
</P>
<P>
<S ID='S-20' IA='BKG' AZ='OTH'> In this definition R acts as a specification of the accessibility relationships which can hold between nodes of the trees admitted by the grammar . </S>
<S ID='S-21' IA='BKG' AZ='OTH'> The rule probabilities specify the cost of making particular choices about the way in which the rules develop . </S>
<S ID='S-22' IA='BKG' AZ='OWN'> It is going to turn out that an exactly analogous system of accessibility relations is present in the probabilistic type hierarchies which we define later . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-2'> Limitations of PCFGs </HEADER>
<P>
<S ID='S-23' IA='BKG' AZ='OTH'> The definition of PCFGs implies that the probability of a phrase marker depends only on the choice of rules used in expanding non-terminal nodes . </S>
<S ID='S-24' IA='BKG' AZ='OTH'> In particular , the probability does not depend on the order in which the rules are applied . </S>
<S ID='S-25' IA='BKG' AZ='CTR' R='CTR'> This has the arguably unwelcome consequence that PCFGs are unable to make certain discriminations between trees which differ only in their configuration . </S>
<S ID='S-26' IA='BKG' AZ='OWN'> The models developed in this paper build in similar independence assumptions . </S>
<S ID='S-27' IA='BKG' AZ='BKG'> A large part of the art of probabilistic language modelling resides in the management of the trade-off between descriptive power ( which has the merit of allowing us to make the discriminations which we want ) and independence assumptions ( which have the merit of making training practical by allowing us to treat similar situations as equivalent ) . </S>
</P>
<P>
<S ID='S-28' IA='BKG' AZ='OTH'> The crucial advantage of PCFGs over CFGs is that they can be trained and / or learned from corpora . </S>
<S ID='S-29' IA='BKG' AZ='OTH'> Readers for whom this fact is unfamiliar are referred to <REFAUTHOR>Charniak</REFAUTHOR> 's textbook <REF TYPE='P'>Charniak 1993</REF> , Chapter 7 . </S>
<S ID='S-30' IA='BKG' AZ='TXT'> We do not have space to recapitulate the discussion of training which can be found there . </S>
<S ID='S-31' IA='BKG' AZ='TXT'> We do however illustrate the outcome of training . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Applying a PCFG to a Simple Corpus </HEADER>
<P>
<S ID='S-32' IA='BKG' AZ='OTH'> Consider the simple grammar in figure <CREF/> and its training against the corpus in figure <CREF/> . </S>
</P>
<IMAGE ID='I-0'/>
<P>
<S ID='S-33' IA='BKG' AZ='OTH'> Since there are 3 plural sentences and only 2 singular sentences , the optimal set of parameters will reflect the distribution found in the corpus , as shown in figure <CREF/> . </S>
</P>
<IMAGE ID='I-1'/>
<P>
<S ID='S-34' IA='BKG' AZ='OTH'> One might have hoped that the ratio <EQN/> would be 2/3 , but it is instead <EQN/> . </S>
<S ID='S-35' IA='BKG' AZ='OTH'> This is a consequence of the assumption of independence . </S>
<S ID='S-36' IA='BKG' AZ='OTH'> Effectively the algorithm is ascribing the difference in distribution of singular and plural sentences to the joint effect of two independent decisions . </S>
<S ID='S-37' IA='BKG' AZ='OWN'> What we would really like it to do is to recognize that the two apparently independent decisions are ( in effect ) one and the same . </S>
<S ID='S-38' IA='BKG' AZ='CTR' R='CTR'> Also , because the grammar has no means of enforcing number agreement , the system systematically prefers plurals to singulars , even when doing this will lead to agreement clashes . </S>
<S ID='S-39' IA='BKG' AZ='CTR'> Thus `` buses stop '' has estimated <EQN/> , `` bus stop '' and `` buses stops '' both have probability <EQN/> and `` bus stops '' has probability <EQN/> . </S>
<S ID='S-40' IA='BKG' AZ='CTR' R='CTR'> This behaviour is clearly unmotivated by the corpus , and arises purely because of the inadequacy of the probabilistic model . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-4'> Probabilistic type hierarchies </HEADER>
<DIV DEPTH='2'>
<HEADER ID='H-5'> ALE Signatures </HEADER>
<P>
<S ID='S-41' IA='BKG' AZ='OTH'> <REFAUTHOR>Carpenter</REFAUTHOR> 's ALE <REF TYPE='P'>Carpenter 1993</REF> allows the user to define the type hierarchy of a grammar by writing a collection of clauses which together denote an inheritance hierarchy , a set of features and a set of appropriateness conditions . </S>
<S ID='S-42' IA='BKG' AZ='OTH'> An example of such a hierarchy is given in ALE syntax in figure <CREF/> . </S>
</P>
<IMAGE ID='I-2'/>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-6'> What the Ale Signature Tells Us  </HEADER>
<P>
<S ID='S-43' IA='BKG' AZ='OTH'> The inheritance information tells us that a sign is a forced choice between a sentence and a phrase , that a phrase is a forced choice between a noun-phrase ( np ) and a verb-phrase ( vp ) and that number values ( num ) are partitioned into singular ( sing ) and plural ( pl ) . </S>
<S ID='S-44' IA='BKG' AZ='OTH'> The features which are defined are left , right , and num , and the appropriateness information says that the feature num introduces a new instance of the type num on all phrases , and that left and right introduce np and vp respectively on sentences . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-7'> The parallel with PCFGs </HEADER>
<P>
<S ID='S-45' IA='BKG' AZ='OWN'> The parallel which makes it possible to apply the PCFG training scheme almost unchanged is that the sub-types of a given super-type partition the feature structures of that type in just the same way that the different rules which expand a given non-terminal N of the PCFG partition the space of trees whose topmost node is N . </S>
<S ID='S-46' IA='BKG' AZ='OWN'> Equally , the features defined in the hierarchy act as an accessibility relation between nodes in a way which is for our purposes entirely equivalent to the way in which the right hand sides of the rules introduce new nodes into partial phrase markers . </S>
<S ID='S-47' IA='BKG' AZ='OWN'> The hierarchy in figure <CREF/> is related to but not isomorphic with the grammar in figure <CREF/> . </S>
</P>
<P>
<S ID='S-48' IA='BKG' AZ='OWN'> One difference is that num is explicitly introduced as a feature in the hierarchy , where at is only implicitly present in the original grammar . </S>
<S ID='S-49' IA='BKG' AZ='OWN'> The other difference is the use of left and right as models of the dominance relationships between nodes . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-8'> A probabilistic interpretation of typed feature-structures </HEADER>
<P>
<S ID='S-50' IA='OWN' AZ='OWN'> For our purposes , a probabilistic type hierarchy ( PTH ) is a four-tuple  </S>
</P>
<IMAGE ID='I-3'/>
<P>
<S ID='S-51' IA='OWN' AZ='OWN'> where MT is a set of maximal types <EQN/> , NT is a set of non-maximal types <EQN/> , <EQN/> is the starting symbol and I is a set of introduction relationships of the form <EQN/> , where <EQN/> is a multiset of maximal and non-maximal types . </S>
<S ID='S-52' IA='OWN' AZ='OWN'> Each introduction relationship has a probability <EQN/> and the probabilities for all the introduction relationships that apply to a given non-maximal type must sum to one . </S>
</P>
<P>
<S ID='S-53' IA='OWN' AZ='OWN'> As things stand this definition is nearly isomorphic to that given for PCFGs , with the major differences being two changes which move us from rules to introduction relationships . </S>
<S ID='S-54' IA='OWN' AZ='OWN'> Firstly , we relax the stipulation that the items on the right hand side of the rules are strings , allowing them instead to be multisets . </S>
<S ID='S-55' IA='OWN' AZ='OWN'> Secondly , we introduce an additional term in the head of introduction rules to signal the fact that when we apply a particular introduction relationship to a node we also specialize the type of the node by picking exactly one of the direct subtypes of its current type . </S>
<S ID='S-56' IA='OWN' AZ='OWN'> Finally , we need to deal with the case where <EQN/> is non-maximal . </S>
<S ID='S-57' IA='OWN' AZ='OWN'> This is simply achieved by defining the iterated introduction relationships from <EQN/> as being those corresponding to the chains of introduction relationships from <EQN/> which refine the type to a maximal type . </S>
<S ID='S-58' IA='OWN' AZ='OWN'> In the probabilistic type hierarchy , it is the iterated introduction relationships which correspond to the context-free rewrite rules of a PCFG . </S>
<S ID='S-59' IA='OWN' AZ='OWN'> A useful side-effect of this is that we can preserve the invariant that all types except those at the fringe of the structure are maximal . </S>
</P>
<P>
<S ID='S-60' IA='OWN' AZ='OWN'> The hierarchy whose ALE syntax is given in figure <CREF/> is captured in the new notation by figure <CREF/> . </S>
</P>
<IMAGE ID='I-4'/>
<P>
<S ID='S-61' IA='OWN' AZ='OWN'> We associate probabilities with feature structures , which are sets of maximal and non-maximal nodes generated by beginning from the starting node and successively expanding non-maximal leaves of the partial tree . </S>
<S ID='S-62' IA='OWN' AZ='OWN'> Maximally specified feature structures are those feature structures which have only maximal leaves . </S>
<S ID='S-63' IA='OWN' AZ='OWN'> Probabilities are assigned by the following inductive definition : </S>
</P>
<P>
<S ID='S-64' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> <EQN/> </S>
<S ID='S-65' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> If F is a feature structure , and F ' is a partial feature structure which differs from it only in that a single non-maximal node <EQN/> of type <EQN/> in F has been refined to type <EQN/> expanded to <EQN/> in F ' , then <EQN/> . </S>
</P>
<P>
<S ID='S-66' IA='OWN' AZ='OWN'> Modulo notation , this definition is identical to the one given earlier for PCFGs . </S>
<S ID='S-67' IA='OWN' AZ='OWN'> Given the correspondence between the definitions of a PTH and a PCFG it should be apparent that the training methods which apply to one can equally be used with the other . </S>
<S ID='S-68' IA='OWN' AZ='OWN'> We will shortly provide an example . </S>
<S ID='S-69' IA='OWN' AZ='OWN'> Because we have not yet treated the crucial matter of re-entrancy , it would be inappropriate to call what we so far have stochastic HPSG , so we refer to it as stochastic <EQN/> . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-9'> Using stochastic HPSG- with the corpus </HEADER>
<P>
<S ID='S-70' IA='OWN' AZ='OWN'> Using the hierarchy in figure <CREF/> the analyses of the five sentences from figure <CREF/> are as in figure <CREF/> . </S>
</P>
<IMAGE ID='I-5'/>
<P>
<S ID='S-71' IA='OWN' AZ='OWN' R='OWN'> Training is a matter of counting the transitions which are found the observed results , then using counts to refine initial estimates of the probabilities of particular transitions . </S>
<S ID='S-72' IA='OWN' AZ='OWN'> This is entirely analogous to what went on with PCFGs . </S>
<S ID='S-73' IA='OWN' AZ='OWN'> The results of training are essentially identical to those given earlier , with the optimal assignment being as shown in figure <CREF/> . </S>
<S ID='S-74' IA='OWN' AZ='OWN'> At this point we have provided a system which allows us to use feature structures instead of PCFGs , but we have not yet dealt with the question of re-entrancy , which forms a crucial part of the expressive power of typed feature structures . </S>
<S ID='S-75' IA='OWN' AZ='OWN'> We will return to this shortly , but first we consider the detailed implications of what we have done so far . </S>
</P>
<IMAGE ID='I-6'/>
<P>
<S ID='S-76' IA='OWN' AZ='OWN'> The similarities between these results and those in figure <CREF/></S>
</P>
<P>
<S ID='S-77' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'>  We still model the distribution observed in the corpus by assuming two independent decisions . </S>
<S ID='S-78' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> We still get a strange ranking of the parses , which favours number disagreement , in spite of the fact that the grammar which generated the corpus enforces number agreement . </S>
</P>
<P>
<S ID='S-79' IA='OWN' AZ='OWN'> The differences between these results and the earlier ones are : </S>
</P>
<P>
<S ID='S-80' IA='OWN' AZ='OWN'> The hierarchy uses bot rather than s as its start symbol . </S>
<S ID='S-81' IA='OWN' AZ='OWN'> The probabilities tell us that the corpus contains no free-standing structures of type num . </S>
</P>
<P>
<S ID='S-82' IA='OWN' AZ='OWN'> The zero probability of </S>
<IMAGE ID='I-7'/>
<S ID='S-83' IA='OWN' AZ='OWN'> codifies a similar observation that there are no free-standing structures with type phrase . </S>
</P>
<P>
<S ID='S-84' IA='OWN' AZ='OWN'> Since items of type phrase are never introduced at that type , but only in the form of sub-types , there are no transitions from phrase in the corpus . </S>
<S ID='S-85' IA='OWN' AZ='OWN'> Therefore the initial estimates of the probabilities of such transitions are unaffected by training . </S>
</P>
<P>
<S ID='S-86' IA='OWN' AZ='OWN'> In the PCFG the symmetry between the expansions of np and vp to singular and plural variants is implicit , whereas in the PTH the distribution of singular and plural variants is encoded at a single location , namely that at which num is refined . </S>
</P>
<P>
<S ID='S-87' IA='OWN' AZ='OWN'> The independence assumption which is built into the training algorithm is that types are to be refined according to the same probability distribution irrespective of the context in which they are expanded . </S>
<S ID='S-88' IA='OWN' AZ='OWN'> We have already seen a consequence of this : the PTH lumps together all occasions where num is expanded , irrespective of whether the enclosing context is np or vp . </S>
<S ID='S-89' IA='OWN' AZ='OWN'> For the moment we are prepared to tolerate this because : </S>
</P>
<P>
<S ID='S-90' IA='OWN' AZ='OWN'> Clarity : </S>
<S ID='S-91' IA='OWN' AZ='OWN'> The decisions which we have made lead to a system with a clear probabilistic semantics . </S>
</P>
<P>
<S ID='S-92' TYPE='ITEM' IA='OWN' AZ='OWN' TYPE='ITEM'> Trainability : </S>
<S ID='S-93' IA='OWN' AZ='OWN'> the number of parameters which must be estimated for a grammar is a linear function of the size of the type hierarchy </S>
</P>
<P>
<S ID='S-94' IA='OWN' AZ='OWN'> Easy extensibility : </S>
<S ID='S-95' IA='OWN' AZ='OWN'> There is a clear route to a more finely grained account if we allow the expansion probabilities to be conditioned on surrounding context . </S>
<S ID='S-96' IA='OWN' AZ='OWN'> This would increase the number of parameters to be estimated , which may or may not prove to be a problem . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-10'> Adding Re-Entrancies </HEADER>
<P>
<S ID='S-97' IA='OWN' AZ='OWN'> We now turn to an extension of the system which takes proper account of re-entrancies in the structure . </S>
<S ID='S-98' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU'> The essence of our approach is to define a stochastic procedure which simultaneously expands the nodes of the tree in the way outlined above and guesses the pattern of re-entrancies which relate them . </S>
<S ID='S-99' IA='OWN' AZ='OWN'> It pays to stipulate that the structures which we build are fully inequated in the sense defined by <REF TYPE='A'>Carpenter 1992</REF> , p120 . </S>
</P>
<P>
<S ID='S-100' IA='OWN' AZ='OWN'> The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes , and this is in turn equivalent to the choice of a partition of the set of nodes into a set of non-empty sets . </S>
<S ID='S-101' IA='OWN' AZ='OWN'> These sets of nodes are equivalence classes . </S>
<S ID='S-102' IA='OWN' AZ='OWN'> The standard recursive procedure for generating partitions of k + 1 elements is to non-deterministically add the k+1thq node to each of the equivalence classes of each of the partitions of k nodes , and also to nondeterministically consider the new node as a singleton set . </S>
<S ID='S-103' IA='OWN' AZ='OWN'> The basis of the stochastic procedure for generating fully-inequated feature structures is to interleave the generation of equivalence classes with the expansion from the initial node as described above . </S>
</P>
<P>
<S ID='S-104' IA='OWN' AZ='OWN'> For the purposes of the expansion algorithm , a fully inequated feature structure consists of a feature tree ( as before ) and an equivalence relation over all the maximal nodes in that tree . </S>
<S ID='S-105' IA='OWN' AZ='OWN'> The task of the algorithm is to generate all such structures and to equip them with probabilities . </S>
<S ID='S-106' IA='OWN' AZ='OWN'> We proceed as in the case without re-entrancy , except that we only ever expand sub-trees in the case where the new node begins a new equivalence class . </S>
<S ID='S-107' IA='OWN' AZ='OWN'> This avoids the double counting which was a problem earlier . </S>
</P>
<P>
<S ID='S-108' IA='OWN' AZ='OWN'> The remaining task is that of assigning scores to equivalence relations . </S>
<S ID='S-109' IA='OWN' AZ='OWN'> We do not have a fully satisfactory solution to this problem . </S>
<S ID='S-110' IA='OWN' AZ='OWN'> The reason for this is that we would ideally like to assign probabilities to intermediate structures in such a way that the probabilities of fully expanded structures are independent of the route by which they were arrived at . </S>
<S ID='S-111' IA='OWN' AZ='OWN'> This can be done , and the method which we adopt has the merit of simplicity . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-11'> Scoring Re-Entrancies </HEADER>
<P>
<S ID='S-112' IA='OWN' AZ='OWN'> We associate a single probabilistic parameter <EQN/> with each type T , and derive the probability of the structure in which a particular pairwise equation of nodes in type T have been equated by multiplying the probability of the structure in which no decision has been made by <EQN/> . </S>
<S ID='S-113' IA='OWN' AZ='OWN'> We derive the probability of the corresponding inequated structure by multiplying by <EQN/> in an entirely analogous way . </S>
<S ID='S-114' IA='OWN' AZ='OWN'> This ensures that the probabilities of the equated and inequated extensions of the original structure sum to the original probability . </S>
<S ID='S-115' IA='OWN' AZ='OWN'> The cost is a deficiency in modelling , since this takes no account of the fact that token identity of nodes is transitive . </S>
<S ID='S-116' IA='OWN' AZ='OWN'> which are generated . </S>
<S ID='S-117' IA='OWN' AZ='OWN'> As things stand the stochastic procedure is free to generate structures where <EQN/> , <EQN/> but <EQN/> , which are not in fact legal feature structures . </S>
<S ID='S-118' IA='OWN' AZ='OWN'> This leads to distortions of the probability estimates since the training algorithm spends part of its probability mass on impossible structures . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-12'> Evaluation </HEADER>
<P>
<S ID='S-119' IA='OWN' AZ='OWN'> Even a crude account of re-entrancy is better than completely ignoring the issue , and the one proposed gets the right result for cases of double counting such as those discussed above , but it should be obvious that there is room for improvement in the treatment which we provide . </S>
<S ID='S-120' IA='OWN' AZ='OWN'> Intuitively what is required is a parametrisable means of distributing probability mass among the distinct equivalence relations which extend the current structure . </S>
<S ID='S-121' IA='OWN' AZ='OWN'> One attractive possibility would be to enumerate the relations which can be obtained by adding the current node to the various different equivalence classes which are available , apply some scoring function to each class , and then normalize such that the total score over all alternatives is one . </S>
<S ID='S-122' IA='OWN' AZ='OWN'> But this might introduce unpleasant dependencies of the probabilities of feature structures on the order in which the stochastic procedure chooses to expand nodes , because the normalisation is carried out before we have full knowledge of the equivalence classes with which the current node might become associated . </S>
<S ID='S-123' IA='OWN' AZ='OWN'> It may be that an appropriate choice of scoring function will circumvent this difficulty , but this is left as a matter for further research . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-13'> Conclusions </HEADER>
<P>
<S ID='S-124' IA='OWN' AZ='AIM' R='AIM' HUMAN='PUPR_global' START='Y'> We have presented two proposals for the association of probabilities with typed feature-structures of the form used in HPSG . </S>
<S ID='S-125' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_+'> As far as we know these are the most detailed of their type , and the ones which are most likely to be able to exploit standard training and parsing algorithms . </S>
<S ID='S-126' IA='OWN' AZ='OWN'> For typed feature structures lacking re-entrancy we believe our proposal to be the simplest and most natural which is available . </S>
<S ID='S-127' IA='OWN' AZ='OWN'> The proposal for dealing with re-entrancy is less satisfactory but offers a basis for empirical exploration , and has definite advantages over the straightforward use of PCFGs . </S>
<S ID='S-128' IA='OWN' AZ='OWN'> We plan to follow up the current work by training and testing a suitable instantiation of our framework against manually annotated corpora . </S>
</P>
</DIV>
</BODY>
<REFERENCES>
<REFERENCE>
Bob <SURNAME>Carpenter</SURNAME>.
<DATE>1992</DATE>.
The Logic of Typed Feature Structures.
Cambridge Tracts in Theoretical Computer Science. CUP.
With Applications to Unification Grammars, Logic Programs and
  Constraint Resolution.
</REFERENCE>
<REFERENCE>
Bob <SURNAME>Carpenter</SURNAME>, <DATE>1993</DATE>.
ALE. The Attribute Logic Engine user's guide, version <EQN/> .
Carnegie Mellon University, Pittsburgh, Pa., Laboratory for
  Computational Linguistics, MAY.
</REFERENCE>
<REFERENCE>
Eugene <SURNAME>Charniak</SURNAME>.
<DATE>1993</DATE>.
Statistical Language Learning.
The MIT Press.
</REFERENCE>
<REFERENCE>
Albert <SURNAME>Kim</SURNAME>.
<DATE>1994</DATE>.
Graded unification: A framework for interactive processing.
In Proceedings of the 32nd Annual Meeting of the Association for
  Computational Linguistics, pages 313-315, June.
</REFERENCE>
<REFERENCE>
C.S. <SURNAME>Mellish</SURNAME>.
<DATE>1988</DATE>.
Implementing systemic classification by unification.
Computational Linguistics, 14(1):40-51.
Winter.
</REFERENCE>
<REFERENCE>
Carl <SURNAME>Pollard</SURNAME> and Ivan A. <SURNAME>Sag</SURNAME>.
<DATE>1994</DATE>.
Head-Driven Phrase Structure Grammar.
CSLI and University of Chicago Press, Stanford, Ca. and Chicago,
  Ill.
</REFERENCE>
</REFERENCES>
</PAPER>
