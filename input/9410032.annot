<?xml version='1.0' encoding='ISO-8859-1'?>
<PAPER>
<METADATA>
<FILENO>9410032</FILENO>
<TITLE> Planning Argumentative Texts </TITLE>
<REFLABEL>Huang 1994c</REFLABEL>
<AUTHORS>
<AUTHOR>Xiaorong Huang</AUTHOR>
</AUTHORS>
<APPEARED><CONFERENCE>COLING</CONFERENCE><YEAR>1994</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.Dc </CLASSIFICATION>
</METADATA>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-0' IA='OWN' AZ='AIM'> This paper presents PROVERB a text planner for argumentative texts . </A-S>
<A-S ID='A-1' DOCUMENTC='S-16' IA='OWN' AZ='AIM'> PROVERB main feature is that it combines global hierarchical planning and unplanned organization of text with respect to local derivation relations in a complementary way . </A-S>
<A-S ID='A-2' DOCUMENTC='S-17' IA='OWN' AZ='OWN'> The former splits the task of presenting a particular proof into subtasks of presenting subproofs . </A-S>
<A-S ID='A-3' DOCUMENTC='S-19' IA='OWN' AZ='OWN'> The latter simulates how the next intermediate conclusion to be presented is chosen under the guidance of the local focus . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' ABSTRACTC='A-0' IA='OWN' AZ='AIM' R='AIM' HUMAN='PUPR_contrib' START='Y'> This paper presents a text planner for the verbalization of natural deduction ( ND ) style proofs <REF TYPE='P'>Gentzen 1935</REF> . </S>
<S ID='S-1' IA='OTH' AZ='OTH'> Several similar attempts can be found in previous work . </S>
<S ID='S-2' IA='OTH' AZ='CTR' R='CTR'> Developed before the era of NL generation , the system EXPOUND of <REF TYPE='A'>Chester 1976</REF> can be characterized as an example of direct translation : Although a sophisticated linearization is applied on the input ND proofs , the steps are then translated locally in a template driven way . </S>
<S ID='S-3' IA='OTH' AZ='CTR' R='CTR'> ND proofs were tested as input to an early version of the MUMBLE system of <REF TYPE='A'>McDonald 1983</REF> , the main aim however , was to show the feasibility of the architecture . </S>
<S ID='S-4' IA='OTH' AZ='CTR' R='CTR'> A more recent attempt can be found in THINKER <REF TYPE='P'>Edgar and Pelletier 1993</REF> , which implements several interesting but isolated proof presentation strategies , without giving a comprehensive underlying model . </S>
</P>
<P>
<S ID='S-5' IA='OWN' AZ='AIM' R='AIM' HUMAN='PUPR;SOLU_prop' START='Y'> Our computational model can therefore be viewed as the first serious attempt at a comprehensive computational model that produces adequate argumentative texts from ND style proofs . </S>
<S ID='S-6' IA='OWN' AZ='AIM' HUMAN='PUPR'> The main aim is to show how existing text planning techniques can be adapted for this particular application . </S>
<S ID='S-7' IA='OWN' AZ='OWN'> To test its feasibility , this computational model is implemented in a system called PROVERB . </S>
</P>
<P>
<S ID='S-8' IA='OTH' AZ='OTH' R='OTH' HUMAN='RWRK_prev'> Most current NL text planners assume that language generation is planned behavior and therefore adopt a hierarchical planning approach <REF TYPE='P'>Hovy 1988</REF> , <REF TYPE='P'>Moore 1989</REF> , <REF  TYPE='P'>Dale 1992</REF>, <REF  TYPE='P'>Reithinger 1991</REF> . </S>
<S ID='S-9' IA='OTH' AZ='OTH'> Nonetheless there is psychological evidence that language has an unplanned , spontaneous aspect as well <REF TYPE='P'>Ochs 1979</REF> . </S>
<S ID='S-10' IA='OTH' AZ='OTH'> Based on this observation , researchers have exploited organizing text with respect to some local relations . </S>
<S ID='S-11' IA='OTH' AZ='OTH'> <REF TYPE='A'>Sibun 1990</REF> implemented a system generating descriptions for objects with a strong domain structure , such as houses , chips and families . </S>
<S ID='S-12' IA='OTH' AZ='OTH'> Once a discourse is started , local structures suggest the next objects available . </S>
<S ID='S-13' IA='OTH' AZ='OTH'> Instead of planning globally , short-range strategies are employed to organize a short segment of text . </S>
<S ID='S-14' IA='OTH' AZ='OTH'> From a computational point of view , a hierarchical planner elaborates recursively on the initial communicative goal until the final subgoals can be achieved by applying a primitive operator . </S>
<S ID='S-15' IA='OTH' AZ='OTH'> A text generator based on the local organization , in contrast , repeatedly chooses a part of the remaining task and carries it out . </S>
</P>
<P>
<S ID='S-16' ABSTRACTC='A-1' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_prop' START='Y'> The macroplanner of PROVERB combines hierarchical planning with local organization in a uniform planning framework . </S>
<S ID='S-17' ABSTRACTC='A-2' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU'> The hierarchical planning is realized by so-called top-down presentation operators that split the task of presenting a particular proof into subtasks of presenting subproofs . </S>
<S ID='S-18' IA='OWN' AZ='BAS' R='BAS'> While the overall planning mechanism follows the RST-based planning approach <REF TYPE='P'>Moore 1989</REF> , <REF TYPE='P'>Reithinger 1991</REF> , the planning operators more closely resemble the schemata in schema-based planning <REF  TYPE='P'>McKeown 1985</REF>, <REF  TYPE='P'>Paris 1988</REF> . </S>
<S ID='S-19' ABSTRACTC='A-3' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU'> Bottom-up presentation operators are devised to simulate the unplanned aspect , where the next intermediate conclusion to be presented is chosen under the guidance of the local focus mechanism in a more spontaneous way . </S>
<S ID='S-20' IA='OWN' AZ='OWN'> Since top-down operators embody explicit communicative norms , they are always given a higher priority . </S>
<S ID='S-21' IA='OWN' AZ='OWN'> Only when no top-down presentation operator is applicable , will a bottom-up presentation operator be chosen . </S>
</P>
<P>
<S ID='S-22' IA='OWN' AZ='BAS'> This distinction between planned and unplanned presentation leads to a very natural segmentation of the discourse into an attentional hierarchy , since , following the theory of <REF TYPE='A'>Grosz and Sidner 1986</REF> , there is a one-to-one correspondence between the intentional hierarchy and the attentional hierarchy . </S>
<S ID='S-23' IA='OWN' AZ='OWN'> This attentional hierarchy is used to make reference choices for inference methods and for previously presented intermediate conclusions . </S>
<S ID='S-24' IA='OWN' AZ='OWN'> The inference choices are the main concern of the microplanner of PROVERB <REF TYPE='P' SELF="YES">Huang 1994b</REF> . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> Context of Our Research </HEADER>
<IMAGE ID='I-0'/>
<P>
<S ID='S-25' IA='OWN' AZ='AIM' HUMAN='PUPR_contrib;TOPIC;SOLU_prop'> The text planner discussed in this paper is the macroplanner of PROVERB , which translates machine-found proofs in several steps into natural language . </S>
<S ID='S-26' IA='OWN' AZ='BAS' R='BAS'> PROVERB adopts a reconstructive approach : Once a proof in a machine oriented formalism is generated in the proof development environment <EQN/> - MKRP , a new proof that more resembles those found in mathematical textbooks is reconstructed <REF SELF="YES" TYPE='P'>Huang 1994a</REF> . </S>
<S ID='S-27' IA='OWN' AZ='OWN'> The reconstructed proof is a proof tree , where proof nodes are derived from their children by applying an inference method ( also called a justification ) . </S>
<S ID='S-28' IA='OWN' AZ='OWN'> Most of the steps are justified by the application of a definition or a theorem , the rest are justified by inference rules of the natural deduction ( ND ) calculus , such as the `` Case '' rule . </S>
<S ID='S-29' IA='OWN' AZ='OWN'> Figure <CREF/> is an example of a segment of a possible input proof , where some nodes are labeled for convenience . </S>
</P>
<P>
<S ID='S-30' IA='OWN' AZ='OWN'> The justifications `` Du '' , `` Dsubgr '' , `` Ds '' , `` Dg '' , and `` Tsol '' stand for the definitions of unit element , of subgroup , of subset , of group , and the theorem about solution , respectively . </S>
</P>
<P>
<S ID='S-31' IA='OWN' AZ='OWN'> The input proof tree is also augmented with an ordered list of nodes , being roots of subproofs planned in this order . </S>
<S ID='S-32' IA='OWN' AZ='OWN'> The proof in Figure <CREF/> is associated with the list : ( <CREF/> , <CREF/> , <CREF/> , <CREF/> ) .</S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-2'> The Framework of the Macroplanner </HEADER>
<P>
<S ID='S-33' IA='OWN' AZ='OWN'> The macroplanner of PROVERB elaborates on communicative goals , selects and orders pieces of information to fulfill these goals . </S>
<S ID='S-34' IA='OWN' AZ='OWN'> The output is an ordered sequence of proof communicative act intentions ( PCAs ) . </S>
<S ID='S-35' IA='OWN' AZ='OWN'> PCAs can be viewed as speech acts in our domain of application . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Planning Framework </HEADER>
<P>
<S ID='S-36' IA='OWN' AZ='OWN'> PROVERB combines the two above mentioned presentation modes by encoding communication knowledge for both top-down planning and bottom-up presentation in form of operators in a uniform planning framework . </S>
<S ID='S-37' IA='OWN' AZ='OWN'> Since top-down presentation operators embody explicit communicative norms , they are given a higher priority . </S>
<S ID='S-38' IA='OWN' AZ='OWN'> A bottom-up presentation is chosen only when no top-down presentation operator applies . </S>
<S ID='S-39' IA='OWN' AZ='OWN'> The overall planning framework is realized by the function Present . </S>
<S ID='S-40' IA='OWN' AZ='OWN'> Taking as input a subproof , Present repeatedly executes a basic planning cycle until the input subproof is conveyed . </S>
<S ID='S-41' IA='OWN' AZ='OWN'> Each cycle carries out one presentation operator , where Present always tries first to choose and apply a top-down operator . </S>
<S ID='S-42' IA='OWN' AZ='OWN'> If impossible , a bottom-up operator will be chosen . </S>
<S ID='S-43' IA='OWN' AZ='OWN'> The function Present is first called with the entire proof as the presentation task . </S>
<S ID='S-44' IA='OWN' AZ='OWN'> The execution of a top-down presentation operator may generate subtasks by calling it recursively . </S>
<S ID='S-45' IA='OWN' AZ='OWN'> The discourse produced by each call to Present forms an attentional unit ( compare the subsection below ) . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> The Discourse Model and the Attentional Hierarchy </HEADER>
<P>
<S ID='S-46' IA='OWN' AZ='OWN'> The discourse carried out so far is recorded in a discourse model . </S>
<S ID='S-47' IA='OWN' AZ='OWN'> Rather than recording the semantic objects and their properties , our discourse model consists basically of the part of the input proof tree which has already been conveyed . </S>
<S ID='S-48' IA='OWN' AZ='OWN'> The discourse model is also segmented into an attentional hierarchy , where subproofs posted by a top-down presentation operators as subtasks constitute attentional units . </S>
<S ID='S-49' IA='OWN' AZ='OWN'> The following are some notions useful for the formulation of the presentation operators : </S>
</P>
<P>
<S ID='S-50' TYPE='ITEM' IA='OWN' AZ='OWN'> Task is the subproof in the input proof whose presentation is the current task . </S>
<S ID='S-51' TYPE='ITEM' IA='OWN' AZ='OWN'> Local focus is the intermediate conclusion last presented , while the semantic objects involved in the local focus are called the focal centers . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-5'> Proof Communicative Acts </HEADER>
<P>
<S ID='S-52' IA='OWN' AZ='OWN'> PCAs are the primitive actions planned during the macroplanning to achieve communicative goals . </S>
<S ID='S-53' IA='OWN' AZ='OWN'> Like speech acts , PCAs can be defined in terms of the communicative goals they fulfill as well as their possible verbalizations . </S>
<S ID='S-54' IA='OWN' AZ='OWN'> Based on an analysis of proofs in mathematical textbooks , each PCA has as goal a combination of the following subgoals : </S>
</P>
<P>
<S ID='S-55' IA='OWN' AZ='OWN'> Conveying a step of the derivation . </S>
<S ID='S-56' IA='OWN' AZ='OWN'> The simplest PCA is the operator Derive . </S>
<S ID='S-57' IA='OWN' AZ='OWN'> Instantiated as below : </S>
<IMAGE ID='I-1'/>
<S ID='S-58' IA='OWN' AZ='OWN'> depending on the reference choices , a possible verbalization is given as following : </S>
<S ID='S-59' IA='OWN' AZ='OWN'> `` Because a is an element of <EQN/> and <EQN/> is a subset of <EQN/> , according to the definition of subset , a is an element of <EQN/> . '' </S>
</P>
<P>
<S ID='S-60' IA='OWN' AZ='OWN'> Updates of the global attentional structure . </S>
<S ID='S-61' IA='OWN' AZ='OWN'> These PCAs sometimes also convey a partial plan for the further presentation . </S>
<S ID='S-62' IA='OWN' AZ='OWN'> Effects of this group of PCAs include : creating new attentional units , setting up partially premises and the goal of a new unit , closing the current unit , or reallocating the attention of the reader from one attentional unit to another . </S>
<S ID='S-63' IA='OWN' AZ='OWN'> The PCA  </S>
<IMAGE ID='I-2'/>
<S ID='S-64' IA='OWN' AZ='OWN'> creates two attentional units with A and B as the assumptions , and Formula as the goal by producing the verbalization : </S>
<S ID='S-65' IA='OWN' AZ='OWN'> `` To prove Formula , let us consider the two cases by assuming A and B. '' </S>
</P>
<P>
<S ID='S-66' IA='OWN' AZ='OWN'> Thirteen PCAs are currently employed in PROVERB . </S>
<S ID='S-67' IA='OWN' AZ='OWN'> See <REF TYPE='A' SELF="YES">Huang 1994b</REF> for more details . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-6'> Structure of the Planning Operators </HEADER>
<P>
<S ID='S-68' IA='OWN' AZ='BAS'> Although top-down and bottom-up presentation activities are of a conceptually different nature , the corresponding communication knowledge is uniformly encoded as presentation operators in a planning framework , similar to the plan operators in other generation systems <REF TYPE='P'>Hovy 1988</REF> , <REF TYPE='P'>Moore 1989</REF> , <REF  TYPE='P'>Dale 1992</REF>, <REF  TYPE='P'>Reithinger 1991</REF> . </S>
<S ID='S-69' IA='OWN' AZ='OWN'> In general , presentation operators map an original presentation task into a sequence of subtasks and finally into a sequence of PCAs . </S>
<S ID='S-70' IA='OWN' AZ='OWN'> All of them have the following four slots : </S>
</P>
<P>
<S ID='S-71' IA='OWN' AZ='OWN'> Proof : a proof schema , which characterizes the syntactical structure of a proof segment for which this operator is designed . </S>
<S ID='S-72' IA='OWN' AZ='OWN'> It plays the role of the goal slot in the traditional planning framework . </S>
</P>
<P>
<S ID='S-73' IA='OWN' AZ='OWN'> Applicability Condition : a predicate . </S>
</P>
<P>
<S ID='S-74' IA='OWN' AZ='OWN'> Acts : a procedure which essentially carries out a sequence of presentation acts . </S>
<S ID='S-75' IA='OWN' AZ='OWN'> They are either primitive PCAs , or are recursive calls to the procedure Present for subproofs . </S>
</P>
<P>
<S ID='S-76' IA='OWN' AZ='OWN'> Features : a list of features which helps to select the best of a set of applicable operators . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-7'> Top-Down Planning </HEADER>
<P>
<S ID='S-77' IA='OWN' AZ='TXT' R='OWN'> This section elaborates on the communicative norms concerning how a proof to be presented can be split into subproofs , as well as how the hierarchically-structured subproofs can be mapped onto some linear order for presentation . </S>
<S ID='S-78' IA='OWN' AZ='CTR' R='CTR'> In contrast with operators employed in RST-based planners that split goals according to the rhetorical structures , our operators encode standard schemata for presenting proofs , which contain subgoals . </S>
<S ID='S-79' IA='OWN' AZ='OWN' TYPE='ITEM'> The top-down presentation operators are roughly divided into two categories : </S>
</P>
<P>
<S ID='S-80' TYPE='ITEM' IA='OWN' AZ='OWN'> schemata-based operators encoding complex schemata for the presentation of proofs of a specific pattern ( twelve of them are currently integrated in PROVERB ) , </S>
<S ID='S-81' TYPE='ITEM' IA='OWN' AZ='OWN'> general operators embodying general presentation norms , concerning splitting proofs and ordering subgoals . </S>
<IMAGE ID='I-3'/>
</P>
<P>
<S ID='S-82' IA='OWN' AZ='OWN'> Let us first look at an operator devised for proof segments containing cases . </S>
<S ID='S-83' IA='OWN' AZ='OWN'> The corresponding schema of such a proof tree is shown in Figure <CREF/> . </S>
<S ID='S-84' IA='OWN' AZ='OWN'> Under two circumstances a writer may recognize that he is confronted with a proof segment containing cases . </S>
<S ID='S-85' IA='OWN' AZ='OWN'> First , when the subproof that has the structure of Figure <CREF/> is the current presentation task , tested by ( task <EQN/> ) . </S>
<S ID='S-86' IA='OWN' AZ='OWN'> Second , when the disjunction <EQN/> has just been presented in the bottom-up mode , tested by ( local-focus <EQN/> ) . </S>
<S ID='S-87' IA='OWN' AZ='OWN'> Under both circumstances , a communication norm motivates the writer to first present the part leading to <EQN/> ( in the second case this subgoal has already been achieved ) , and then to proceed with the two cases . </S>
<S ID='S-88' IA='OWN' AZ='OWN'> It enforces also that certain PCAs be used to mediate between parts of proofs . </S>
<S ID='S-89' IA='OWN' AZ='OWN'> This procedure is exactly captured by the presentation operator below . </S>
</P>
<P>
<S ID='S-90' IA='OWN' AZ='OWN'> Case-Implicit </S>
</P>
<P>
<S ID='S-91' IA='OWN' AZ='OWN'> Proof : as given in Figure <CREF/> </S>
</P>
<P>
<S ID='S-92' IA='OWN' AZ='OWN'> Applicability Condition : <EQN/></S>
</P>
<P>
<S ID='S-93' IA='OWN' AZ='OWN'> Acts : </S>
<S ID='S-94' TYPE='ITEM' IA='OWN' AZ='OWN'> if <EQN/> has not been conveyed , then present <EQN/> ( subgoal 1 ) </S>
<S ID='S-95' TYPE='ITEM' IA='OWN' AZ='OWN'> a PCA with the verbalization : `` First , let us consider the first case by assuming F. '' </S>
<S ID='S-96' TYPE='ITEM' IA='OWN' AZ='OWN'> present <EQN/> ( subgoal 2 ) a PCA with the verbalization : `` Next , we consider the second case by assuming G . '' </S>
<S ID='S-97' TYPE='ITEM' IA='OWN' AZ='OWN'> present <EQN/> ( subgoal 3 ) mark <EQN/> as conveyed </S>
</P>
<P>
<S ID='S-98' IA='OWN' AZ='OWN'> features : ( top-down compulsory implicit ) . </S>
</P>
<P>
<S ID='S-99' IA='OWN' AZ='OWN'> The feature values can be divided into two groups : those characterizing the style of the text this operator produces , and those concerning other planning aspects . </S>
<S ID='S-100' IA='OWN' AZ='OWN'> `` Implicit '' is a stylistic feature value , indicating that the splitting of the proof into the three subgoals is not made explicit . </S>
<S ID='S-101' IA='OWN' AZ='OWN'> In its explicit dual Case-Explicit a PCA is added to the beginning of the Acts slot , which produces the verbalization : </S>
</P>
<P>
<S ID='S-102' IA='OWN' AZ='OWN'> `` To prove Q , let us first prove <EQN/> , and consider the two cases separately . '' </S>
</P>
<P>
<S ID='S-103' IA='OWN' AZ='OWN'> The feature value `` compulsory '' indicates that if the applicability condition is satisfied , and the style of the operator conforms to the global style the text planner is committed to , this operator should be chosen . </S>
<S ID='S-104' IA='OWN' AZ='OWN'> Two weaker values also reflect the specificity of plan operators : `` specific '' and `` general '' . </S>
</P>
<P>
<S ID='S-105' IA='OWN' AZ='OWN'> General presentation operators perform a simple task according to some general text organization principles . </S>
<S ID='S-106' IA='OWN' AZ='OWN' TYPE='ITEM'> They either </S>
<S ID='S-107' TYPE='ITEM' IA='OWN' AZ='OWN'> enforce a linearization on subproofs to be presented , or </S>
<S ID='S-108' TYPE='ITEM' IA='OWN' AZ='OWN'> split the task of the presentation of a proof with ordered subproofs into subtasks . </S>
</P>
<P>
<S ID='S-109' IA='OWN' AZ='OWN'> The first ordering operator operationalizes a general ordering strategy called minimal load principle . </S>
<S ID='S-110' IA='OWN' AZ='OWN'> This principle predicates that a writer usually presents shorter branches before longer ones . </S>
<S ID='S-111' IA='OWN' AZ='OWN'> The argument of Levelt is rather simple : When one branch is chosen to be described first , the writer has to have the choice node flagged in his memory for return . </S>
<S ID='S-112' IA='OWN' AZ='OWN'> If he follows the shorter branch first , the duration of the load will be shorter . </S>
<S ID='S-113' IA='OWN' AZ='OWN'> The concrete operator is omitted . </S>
</P>
<P>
<S ID='S-114' IA='OWN' AZ='OWN'> Note that , the subproofs being ordered are subproofs conceptually planned while the corresponding proof is constructed . </S>
<S ID='S-115' IA='OWN' AZ='OWN'> There are two other ordering operators based on general ordering principles : the local focus principle and the proof time order principle <REF SELF="YES" TYPE='P'>Huang 1994b</REF> . </S>
</P>
<P>
<S ID='S-116' IA='OWN' AZ='OWN'> The invocation of an ordering operator is always followed by the invocation of a splitting operator , which actually posts subgoals by calling the function Present with the ordered goals subsequently . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-8'> Bottom-Up Presentation </HEADER>
<P>
<S ID='S-117' IA='OWN' AZ='OWN'> The bottom-up presentation process simulates the unplanned part of proof presentation . </S>
<S ID='S-118' IA='OWN' AZ='OWN'> Instead of splitting presentation goals into subgoals according to standard schemata , it follows the local derivation relation to find a next proof node or subproof to be presented . </S>
<S ID='S-119' IA='OWN' AZ='BAS' R='BAS'> In this sense , it is similar to the local organization techniques used in <REF TYPE='A'>Sibun 1990</REF> . </S>
<S ID='S-120' IA='OWN' AZ='OWN'> When no top-down presentation operator applies , PROVERB chooses a bottom-up operator . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-9'> The Local Focus </HEADER>
<P>
<S ID='S-121' IA='OWN' AZ='OWN'> The node to be presented next is suggested by the mechanism of local focus . </S>
<S ID='S-122' IA='OWN' AZ='OWN'> Although logically any proof node having the local focus as a child could be chosen for the next step , usually the one with the greatest semantic overlapping with the focal centers is preferred . </S>
<S ID='S-123' IA='OWN' AZ='OWN'> As mentioned above , focal centers are semantic objects mentioned in the proof node which is the local focus . </S>
<S ID='S-124' IA='OWN' AZ='OWN'> This is based on the observation that if one has proved a property about some semantic objects , one tends to continue to talk about these particular objects before turning to new objects . </S>
<S ID='S-125' IA='OWN' AZ='OWN'> Let us examine the situation when the proof below is awaiting presentation . </S>
<IMAGE ID='I-4'/>
</P>
<P>
<S ID='S-126' IA='OWN' AZ='OWN'> Assume that node <CREF/> is the local focus , the set <EQN/> are the focal centers , <EQN/> is a previously presented node and node <CREF/> is the current task . </S>
<S ID='S-127' IA='OWN' AZ='OWN'> <CREF/> is chosen as the next node to be presented , since it does not ( re ) introduce any new semantic object and its overlap with the focal centers ( <EQN/> ) is larger than those of <CREF/> ( <EQN/> ) . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-10'> The Bottom-Up Presentation Operators </HEADER>
<P>
<S ID='S-128' IA='OWN' AZ='OWN'> Under different circumstances the derivation of the next-node is also presented in different ways . </S>
<S ID='S-129' IA='OWN' AZ='OWN'> The corresponding presentation knowledge is encoded as bottom-up presentation operators . </S>
<S ID='S-130' IA='OWN' AZ='OWN'> The one most frequently used presents one step of derivation : </S>
</P>
<P>
<S ID='S-131' TYPE='ITEM' IA='OWN' AZ='OWN'> Derive-Bottom-Up . </S>
<S ID='S-132' TYPE='ITEM' IA='OWN' AZ='OWN'> Proof : <EQN/>  </S>
<S ID='S-133' TYPE='ITEM' IA='OWN' AZ='OWN'> Applicability Condition : <EQN/> is suggested by the focus mechanism as the next node , and <EQN/> , <EQN/> , <EQN/> are conveyed . </S>
<S ID='S-134' TYPE='ITEM' IA='OWN' AZ='OWN'> Acts : a PCA that conveys the fact that <EQN/> is derived from the premises <EQN/> , <EQN/> , <EQN/> by applying <EQN/> . </S>
<S ID='S-135' TYPE='ITEM' IA='OWN' AZ='OWN'> Features : ( bottom-up general explicit detailed ) . </S>
</P>
<P>
<S ID='S-136' IA='OWN' AZ='OWN'> If the conclusion <EQN/> , the premises and the method <EQN/> are instantiated to <EQN/> , ( <EQN/> , <EQN/> ) , def-subset respectively , the following verbalization can be produced : </S>
</P>
<P>
<S ID='S-137' IA='OWN' AZ='OWN'> `` Since a is an element of <EQN/> , and <EQN/> is a subset of <EQN/> , a is an element of <EQN/> according to the definition of subset . '' </S>
</P>
<P>
<S ID='S-138' IA='OWN' AZ='OWN'> A trivial subproof may be presented as a single derivation by omitting the intermediate nodes . </S>
<S ID='S-139' IA='OWN' AZ='OWN'> This next subproof is also suggested by the local focus . </S>
<S ID='S-140' IA='OWN' AZ='OWN'> This is simulated by a bottom-up operator called Simplify-Bottom-Up . </S>
<S ID='S-141' IA='OWN' AZ='OWN'> Currently seven bottom-up operators are integrated in PROVERB . </S>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-11'> Verbalization of PCAs </HEADER>
<P>
<S ID='S-142' IA='OWN' AZ='OWN'> Macroplanning produces a sequence of PCAs . </S>
<S ID='S-143' IA='OWN' AZ='OWN'> Our microplanner is restricted to the treatment of the reference choices for the inference methods and for the previously presented intermediate conclusions . </S>
<S ID='S-144' IA='OWN' AZ='OWN'> While the former depends on static salience relating to the domain knowledge , the latter is similar to subsequent references , and is therefore sensitive to the context , in particular to its segmentation into attentional hierarchy . </S>
<S ID='S-145' IA='OWN' AZ='OWN'> Due to space restrictions , we only show the following piece of a preverbal message as an example , being a PCA enriched with reference choices for reasons and method by the microplanner <REF  SELF="YES" TYPE='P'>Huang 1994b</REF>, <REF  SELF="YES" TYPE='P'>Huang 1994b</REF> . </S>
<IMAGE ID='I-5'/>
</P>
<P>
<S ID='S-146' IA='OWN' AZ='OTH'> Our surface generator TAG-GEN <REF TYPE='P'>Kilger 1994</REF> produces the utterance : </S>
</P>
<P>
<S ID='S-147' IA='OWN' AZ='OTH'> `` Since a is an element of U , a is an element of F. ''  </S>
</P>
<P>
<S ID='S-148' IA='OWN' AZ='OTH'> Notice , only the reason labeled as `` explicit '' is verbalized . </S>
</P>
<P>
<S ID='S-149' IA='OWN' AZ='OWN'> Finally , to demonstrate the type of proofs currently generated by PROVERB , below is the complete output for a proof constructed by <EQN/> - MKRP : </S>
</P>
<P>
<S ID='S-150' IA='OWN' AZ='OWN'> Theorem : </S>
<S ID='S-151' IA='OWN' AZ='OWN'> Let F be a group and U a subgroup of F , if 1 and <EQN/> are unit elements of F and U respectively , then <EQN/> . </S>
</P>
<P>
<S ID='S-152' IA='OWN' AZ='OWN'> Proof : </S>
<S ID='S-153' IA='OWN' AZ='OWN'> Let F be a group , U be a subgroup of F , 1 be a unit element of F and <EQN/> be a unit element of U. According to the definition of unit element , <EQN/> . </S>
<S ID='S-154' IA='OWN' AZ='OWN'> Therefore there is an X , <EQN/> . </S>
<S ID='S-155' IA='OWN' AZ='OWN'> Now suppose that <EQN/> is such an X . </S>
<S ID='S-156' IA='OWN' AZ='OWN'> According to the definition of unit element , <EQN/> . </S>
<S ID='S-157' IA='OWN' AZ='OWN'> Since U is a subgroup of F , <EQN/> . </S>
<S ID='S-158' IA='OWN' AZ='OWN'> Therefore <EQN/> . </S>
<S ID='S-159' IA='OWN' AZ='OWN'> Similarly <EQN/> , since <EQN/> . </S>
<S ID='S-160' IA='OWN' AZ='OWN'> Since F is a group , F is a semigroup . </S>
<S ID='S-161' IA='OWN' AZ='OWN'> Because <EQN/> , <EQN/> is a solution of the equation <EQN/> . </S>
<S ID='S-162' IA='OWN' AZ='OWN'> Since 1 is a unit element of F , <EQN/> . </S>
<S ID='S-163' IA='OWN' AZ='OWN'> Since 1 is a unit element of F , <EQN/> . </S>
<S ID='S-164' IA='OWN' AZ='OWN'> Because <EQN/> , 1 is a solution of the equation <EQN/> . </S>
<S ID='S-165' IA='OWN' AZ='OWN'> Since F is a group , <EQN/> by the uniqueness of solution . </S>
<S ID='S-166' IA='OWN' AZ='OWN'> This conclusion is independent of the choice of the element <EQN/> . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-12'> Conclusion and Future Work </HEADER>
<P>
<S ID='S-167' IA='OWN' AZ='AIM' R='AIM' HUMAN='SOLU' START='Y'> This paper puts forward an architecture that combines several established NL generation techniques adapted for a particular application , namely the presentation of ND style proofs . </S>
<S ID='S-168' IA='OWN' AZ='OWN'> We hope that this architecture is also of general interest beyond this particular application . </S>
</P>
<P>
<S ID='S-169' IA='OWN' AZ='AIM' R='AIM' HUMAN='SOLU_prop'> The most important feature of this model is that hierarchical planning and unplanned spontaneous presentation are integrated in a uniform framework . </S>
<S ID='S-170' IA='OWN' AZ='OTH'> Top-down hierarchical planning views language generation as planned behavior . </S>
<S ID='S-171' IA='OWN' AZ='OTH'> Based on explicit communicative knowledge encoded as schemata , hierarchical planning splits a presentation task into subtasks . </S>
<S ID='S-172' IA='OWN' AZ='CTR' R='CTR' HUMAN='SOLU_contrast'> Although our overall presentation mechanism has much in common with that of RST-based text planners , the top-down planning operators contain mostly complex presentation schemata , like those in schema-based planning . </S>
<S ID='S-173' IA='OWN' AZ='OWN'> Since schemata-based planning covers only proofs of some particular structure , it is complemented by a mechanism called bottom-up presentation . </S>
<S ID='S-174' IA='OWN' AZ='OWN'> Bottom-up presentation aims at simulating the unplanned part of proof presentation , where a proof node or a subproof awaiting presentation is chosen as the next to be presented via the local derivation relations . </S>
<S ID='S-175' IA='OWN' AZ='OWN'> Since more than one such node is often available , the local focus mechanism is employed to single out the candidate having the strongest semantic links with the focal centers . </S>
<S ID='S-176' IA='OWN' AZ='OWN'> The distinction between planned and unplanned behavior enables a very natural segmentation of the discourse into an attentional hierarchy . </S>
<S ID='S-177' IA='OWN' AZ='BAS'> This provide an appropriate basis for a discourse theory which handles reference choices <REF SELF="YES" TYPE='P'>Huang 1994b</REF> . </S>
</P>
<P>
<S ID='S-178' IA='OWN' AZ='OWN'> Compared with proofs found in mathematical textbooks , the output of PROVERB is still too tedious and inflexible . </S>
<S ID='S-179' IA='OWN' AZ='OWN'> The tediousness is largely ascribed to the lack of plan level knowledge of the input proofs , which distinguishes crucial steps from unimportant details . </S>
<S ID='S-180' IA='OWN' AZ='OWN'> Therefore , sophisticated plan recognition techniques are necessary . </S>
<S ID='S-181' IA='OWN' AZ='OWN'> The inflexibility of text currently produced is partly inherited from the schemata-based approach , for which a fine-grained planning in terms of single PCAs might be a remedy . </S>
<S ID='S-182' IA='OWN' AZ='OWN'> It is also partly due to the fixed lexicon choice , which we are currently reimplementing . </S>
</P>
</DIV>
</BODY>
<REFERENCES>
<REFERENCE>
D. <SURNAME>Chester</SURNAME>.
The translation of formal proofs into English.
Artificial Intelligence, <DATE>1976</DATE>.
</REFERENCE>
<REFERENCE>
R. <SURNAME>Dale</SURNAME>.
Generating Referring Expressions.
MIT Press, <DATE>1992</DATE>.
</REFERENCE>
<REFERENCE>
A. <SURNAME>Edgar</SURNAME> and F. J. <SURNAME>Pelletier</SURNAME>.
Natural language explanation of natural deduction proofs.
In Proc.the first Conf. of the Pacific Assoc. for
  Comp. Linguistics, <DATE>1993</DATE>.
</REFERENCE>
<REFERENCE>
G. <SURNAME>Gentzen</SURNAME>.
Untersuchungen ber das logische Schlieen I.
Math. Zeitschrift, <DATE>1935</DATE>.
</REFERENCE>
<REFERENCE>
B. J. <SURNAME>Grosz</SURNAME> and C. L. <SURNAME>Sidner</SURNAME>.
Attention, intentions, and the structure of discourse.
Computational Linguistics, <DATE>1986</DATE>.
</REFERENCE>
<REFERENCE>
E. H. <SURNAME>Hovy</SURNAME>.
Generating Natural Language under Progmatic Constrints.
Lawrence Erlbaum Associates, Hillsdale, <DATE>1988</DATE>.
</REFERENCE>
<REFERENCE>
X. <SURNAME>Huang</SURNAME>.
Reconstructing proofs at the assertion level.
In Proc.12th CADE, <DATE>1994</DATE>, <DATE>forthcoming</DATE>.
</REFERENCE>
<REFERENCE>
X. <SURNAME>Huang</SURNAME>.
Planning Reference Choices for Argumentative Texts.
In Proc.the 7th International Workshop on Natural Language Generation, <DATE>1994</DATE>, <DATE>forthcoming</DATE>.
</REFERENCE>
<REFERENCE>
X. <SURNAME>Huang</SURNAME>.
A Reconstructive Approach to Human Oriented Proof Presentation.
PhD thesis, Universitt des Saarlandes, Germany, <DATE>1994</DATE>, <DATE>forthcoming</DATE>.
</REFERENCE>
<REFERENCE>
A. <SURNAME>Kilger</SURNAME>.
Using UTAGs for incremental and parallel generation.
Computational Intelligence, <DATE>forthcoming</DATE>, <DATE>1994</DATE>.
</REFERENCE>
<REFERENCE>
D. D. <SURNAME>McDonald</SURNAME>.
Natural language generation as a computational problem.
In Brady/Berwick: Computational Models of Discourse. MIT Press,
  <DATE>1983</DATE>.
</REFERENCE>
<REFERENCE>
K. R. <SURNAME>McKeown</SURNAME>.
Text Generation.
Cambridge University Press, <DATE>1985</DATE>.
</REFERENCE>
<REFERENCE>
J. D. <SURNAME>Moore</SURNAME>.
A Reactive Approach to Explanation in Expert and Advice-Giving
  Systems.
PhD thesis, Univ. of California, <DATE>1989</DATE>.
</REFERENCE>
<REFERENCE>
E. <SURNAME>Ochs</SURNAME>.
Planned and unplanned discourse.
Syntax and Semantics, <DATE>1979</DATE>.
</REFERENCE>
<REFERENCE>
C. <SURNAME>Paris</SURNAME>.
Tailoring object descriptions to a user's level of expertise.
Computational Linguistics, <DATE>1988</DATE>.
</REFERENCE>
<REFERENCE>
N. <SURNAME>Reithinger</SURNAME>.
Eine parallele Architektur zur inkrementeller Dialogbeitrge.
PhD thesis, Universitt des Saarlandes, <DATE>1991</DATE>.
</REFERENCE>
<REFERENCE>
P. <SURNAME>Sibun</SURNAME>.
The local organization of text.
In K.R. McKeown etal, editors, Proc.
  of the 5th International Workshop on Natural Language Generation, <DATE>1990</DATE>.
</REFERENCE>
</REFERENCES>
</PAPER>
