<?xml version='1.0' encoding='ISO-8859-1'?>
<PAPER>
<METADATA>
<FILENO>9605014</FILENO>
<TITLE> Clustering Words with the MDL Principle </TITLE>
<REFLABEL>Li and Abe 1996b</REFLABEL>
<AUTHORS>
<AUTHOR>Hang Li</AUTHOR>
<AUTHOR>Naoki Abe</AUTHOR>
</AUTHORS>
<APPEARED><CONFERENCE>COLING</CONFERENCE><YEAR>1996</YEAR></APPEARED>
<CLASSIFICATION> Lg.Pr.St </CLASSIFICATION>
</METADATA>
<ABSTRACT>
<A-S ID='A-0' DOCUMENTC='S-156' IA='OWN' AZ='AIM'> We address the problem of automatically constructing a thesaurus by clustering words based on corpus data . </A-S>
<A-S ID='A-1' DOCUMENTC='S-6' IA='OWN' AZ='BAS'> We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs , and propose a learning algorithm based on the Minimum Description Length ( MDL ) Principle for such estimation . </A-S>
<A-S ID='A-2' DOCUMENTC='S-10;S-11' IA='OWN' AZ='CTR' DOCUMENTC='S-10;S-11'> We empirically compared the performance of our method based on the MDL Principle against the Maximum Likelihood Estimator in word clustering , and found that the former outperforms the latter . </A-S>
<A-S ID='A-3' DOCUMENTC='S-12' IA='OWN' AZ='OWN'> We also evaluated the method by conducting pp-attachment disambiguation experiments using an automatically constructed thesaurus . </A-S>
<A-S ID='A-4' DOCUMENTC='S-15' IA='OWN' AZ='OWN'> Our experimental results indicate that such a thesaurus can be used to improve accuracy in disambiguation . </A-S>
</ABSTRACT>
<BODY>
<DIV DEPTH='1'>
<HEADER ID='H-0'> Introduction </HEADER>
<P>
<S ID='S-0' IA='BKG' AZ='OTH' R='OTH' HUMAN='RWRK_prev'> Recently various methods for automatically constructing a thesaurus ( hierarchically clustering words ) based on corpus data have been proposed <REF TYPE='P'>Hindle 1990</REF> , <REF TYPE='P'>Brown et al. 1992</REF> , <REF  TYPE='P'>Pereira et al. 1993</REF>, <REF  TYPE='P'>Tokunaga et al. 1995</REF> . </S>
<S ID='S-1' IA='BKG' AZ='BKG' TYPE='ITEM'> The realization of such an automatic construction method would make it possible to </S>
<S ID='S-2' TYPE='ITEM' IA='BKG' AZ='BKG' TYPE='ITEM'> save the cost of constructing a thesaurus by hand , </S>
<S ID='S-3' TYPE='ITEM' IA='BKG' AZ='BKG' TYPE='ITEM'> do away with subjectivity inherent in a hand made thesaurus , and </S>
<S ID='S-4' TYPE='ITEM' IA='BKG' AZ='BKG' TYPE='ITEM'> make it easier to adapt a natural language processing system to a new domain . </S>
<S ID='S-5' IA='OWN' AZ='AIM' R='AIM' HUMAN='PUPR' START='Y'> In this paper , we propose a new method for automatic construction of thesauri . </S>
<S ID='S-6' ABSTRACTC='A-1' IA='OWN' AZ='BAS' R='BAS' HUMAN='PUPR;SOLU'> Specifically , we view the problem of automatically clustering words as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns ( in general , any set of words ) and a partition of a set of verbs ( in general , any set of words ) , and propose an estimation algorithm using simulated annealing with an energy function based on the Minimum Description Length ( MDL ) Principle . </S>
<S ID='S-7' IA='OTH' AZ='OTH'> The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics . </S>
<S ID='S-8' IA='OTH' AZ='OTH'> As a strategy of statistical estimation MDL is guaranteed to be near optimal . </S>
</P>
<P>
<S ID='S-9' IA='OWN' AZ='OWN' START='Y'> We empirically evaluated the effectiveness of our method . </S>
<S ID='S-10' ABSTRACTC='A-2' IA='OWN' AZ='CTR' R='CTR' HUMAN='SOLU_local'> In particular , we compared the performance of an MDL-based simulated annealing algorithm in hierarchical word clustering against that of one based on the Maximum Likelihood Estimator ( MLE , for short ) . </S>
<S ID='S-11' ABSTRACTC='A-2' IA='OWN' AZ='CTR' R='CTR' HUMAN='CLCO'> We found that the MDL-based method performs better than the MLE-based method . </S>
<S ID='S-12' ABSTRACTC='A-3' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_local'> We also evaluated our method by conducting pp-attachment disambiguation experiments using a thesaurus automatically constructed by it and found that disambiguation results can be improved . </S>
</P>
<P>
<S ID='S-13' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_local;PUPR_local'> Since some words never occur in a corpus , and thus cannot be reliably classified by a method solely based on corpus data , we propose to combine the use of an automatically constructed thesaurus and a hand made thesaurus in disambiguation . </S>
<S ID='S-14' IA='OWN' AZ='OWN'> We conducted some experiments in order to test the effectiveness of this strategy . </S>
<S ID='S-15' ABSTRACTC='A-4' IA='OWN' AZ='OWN' R='OWN' HUMAN='CLCO'> Our experimental results indicate that combining an automatically constructed thesaurus and a hand made thesaurus widens the coverage of our disambiguation method , while maintaining high accuracy . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-1'> The Problem Setting </HEADER>
<P>
<S ID='S-16' IA='OTH' AZ='BKG'> A method of constructing a thesaurus based on corpus data usually consists of the following three steps : </S>
<S ID='S-17' IA='OTH' AZ='BKG'> Extract co-occurrence data ( e.g. case frame data , adjacency data ) from a corpus , </S>
<S ID='S-18' IA='OTH' AZ='BKG'> Starting from a single class ( or each word composing its own class ) , divide ( or merge ) word classes based on the co-occurrence data using some similarity ( distance ) measure . </S>
<S ID='S-19' IA='OTH' AZ='BKG'> ( The former approach is called ` divisive , ' the latter ` agglomerative ' ) </S>
<S ID='S-20' IA='OTH' AZ='BKG'> Repeat step <CREF/> until some stopping condition is met , to construct a thesaurus ( tree ) . </S>
<S ID='S-21' IA='OWN' AZ='OWN'> The method we propose here consists of the same three steps . </S>
</P>
<P>
<S ID='S-22' IA='OWN' AZ='OWN'> Suppose available to us are data like those in Figure <CREF/> , which are frequency data ( co-occurrence data ) between verbs and their objects extracted from a corpus ( step <CREF/> ) . </S>
<S ID='S-23' IA='OWN' AZ='OWN'> We then view the problem of clustering words as that of estimating a probabilistic model ( representing probability distribution ) that generates such data . </S>
<S ID='S-24' IA='OWN' AZ='OWN'> We assume that the target model can be defined in the following way . </S>
<S ID='S-25' IA='OWN' AZ='OWN'> First , we define a noun partition <EQN/> over a given set of nouns <EQN/> and a verb partion <EQN/> over a given set of verbs <EQN/> . </S>
<S ID='S-26' IA='OWN' AZ='OWN'> A noun partition is any set <EQN/> satisfying <EQN/> , <EQN/> and <EQN/> . </S>
<S ID='S-27' IA='OWN' AZ='OWN'> A verb partition <EQN/> is defined analogously . </S>
<S ID='S-28' IA='OWN' AZ='OWN'> In this paper , we call a member of a noun partition a ` noun cluster , ' and a member of a verb partition a ` verb cluster ' . </S>
<S ID='S-29' IA='OWN' AZ='OWN'> We refer to a member of the Cartesian product of a noun partition and a verb partition ( <EQN/> ) simply as a ` cluster ' . </S>
<S ID='S-30' IA='OWN' AZ='OWN'> We then define a probabilistic model ( a joint distribution ) , written <EQN/> , where random variable <EQN/> assumes a value from a fixed noun partition <EQN/> , and <EQN/> a value from a fixed verb partition <EQN/> . </S>
<S ID='S-31' IA='OWN' AZ='OWN'> Within a given cluster , we assume that each element is generated with equal probability , i.e. ,  </S>
<IMAGE ID='I-0'/>
</P>
<P>
<S ID='S-32' IA='OWN' AZ='OWN'> Figure <CREF/> shows two example models which might have given rise to the data in Figure <CREF/> . </S>
</P>
<P>
<S ID='S-33' IA='OWN' AZ='OWN'> In this paper , we assume that the observed data are generated by a model belonging to the class of models just described , and select a model which best explains the data . </S>
<S ID='S-34' IA='OWN' AZ='OWN'> As a result of this , we obtain both noun clusters and verb clusters . </S>
<S ID='S-35' IA='OWN' AZ='OWN'> This problem setting is based on the intuitive assumption that similar words occur in the same context with roughly equal likelihood , as is made explicit in equation <CREF/> . </S>
<S ID='S-36' IA='OWN' AZ='OWN'> Thus selecting a model which best explains the given data is equivalent to finding the most appropriate classification of words based on their co-occurrence . </S>
<IMAGE ID='I-1'/>
<IMAGE ID='I-2'/>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-2'> Clustering with MDL </HEADER>
<P>
<S ID='S-37' IA='OWN' AZ='OWN'> We now turn to the question of what strategy ( or criterion ) we should employ for estimating the best model . </S>
<S ID='S-38' IA='OTH' AZ='BAS' R='BAS'> Our choice is the MDL ( Minimum Description Length ) principle <REF TYPE='P'>Rissanen 1978</REF> , <REF TYPE='P'>Rissanen 1983</REF> , <REF TYPE='P'>Rissanen 1984</REF> , <REF TYPE='P'>Rissanen 1986</REF> , <REF TYPE='P'>Rissanen 1989</REF> , a well-known principle of data compression and statistical estimation from information theory . </S>
<S ID='S-39' IA='OTH' AZ='OTH'> MDL stipulates that the best probability model for given data is that model which requires the least code length for encoding of the model itself , as well as the given data relative to it . </S>
<S ID='S-40' IA='OTH' AZ='OWN'> We refer to the code length for the model as the ` model description length ' and that for the data the ` data description length ' . </S>
</P>
<P>
<S ID='S-41' IA='OWN' AZ='BAS' R='BAS' START='Y'> We apply MDL to the problem of estimating a model consisting of a pair of partitions as described above . </S>
<S ID='S-42' IA='OWN' AZ='BKG'> In this context , a model with less clusters , such as Model 2 in Figure <CREF/> , tends to be simpler ( in terms of the number of parameters ) , but also tends to have a poorer fit to the data . </S>
<S ID='S-43' IA='OWN' AZ='BKG'> In contrast , a model with more clusters , such as Model 1 in Figure <CREF/> , is more complex , but tends to have a better fit to the data . </S>
<S ID='S-44' IA='OWN' AZ='BKG'> Thus , there is a trade-off relationship between the simplicity of clustering ( a model ) and the goodness of fit to the data . </S>
<S ID='S-45' IA='OWN' AZ='OTH'> The model description length quantifies the simplicity ( complexity ) of a model , and the data description length quantifies the fit to the data . </S>
<S ID='S-46' IA='OWN' AZ='OTH'> According to MDL , the model which minimizes the sum total of the two types of description lengths should be selected . </S>
</P>
<P>
<S ID='S-47' IA='OWN' AZ='TXT'> In what follows , we will describe in detail how the description length is to be calculated in our current context , as well as our simulated annealing algorithm based on MDL . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-3'> Calculating Description Length </HEADER>
<P>
<S ID='S-48' IA='OWN' AZ='TXT' R='OWN'> We will now describe how the description length for a model is calculated . </S>
<S ID='S-49' IA='OWN' AZ='OWN'> Recall that each model is specified by the Cartesian product of a noun partition and a verb partition , and a number of parameters for them . </S>
<S ID='S-50' IA='OWN' AZ='OWN'> Here we let <EQN/> denote the size of the noun partition , and <EQN/> the size of the verb partition . </S>
<S ID='S-51' IA='OWN' AZ='OWN'> Then , there are <EQN/> free parameters in a model . </S>
</P>
<P>
<S ID='S-52' IA='OWN' AZ='OWN'> Given a model M and data S , its total description length L ( M ) is computed as the sum of the model description length <EQN/> , the description length of its parameters <EQN/> , and the data description length <EQN/> . </S>
<S ID='S-53' IA='OWN' AZ='OWN'> ( We often refer to <EQN/> as the model description length ) . </S>
<S ID='S-54' IA='OWN' AZ='OWN'> Namely ,  </S>
<IMAGE ID='I-3'/>
</P>
<P>
<S ID='S-55' IA='OWN' AZ='OWN'> We employ the ` binary noun clustering method , ' in which <EQN/> is fixed at <EQN/> and we are to decide whether <EQN/> or <EQN/> , which is then to be applied recursively to the clusters thus obtained . </S>
<S ID='S-56' IA='OWN' AZ='OWN'> This is as if we view the nouns as entities and the verbs as features and cluster the entities based on their features . </S>
<S ID='S-57' IA='OWN' AZ='OWN'> Since there are <EQN/> subsets of the set of nouns <EQN/> , and for each ` binary ' noun partition we have two different subsets ( a special case of which is when one subset is <EQN/> and the other the empty set <EQN/> ) , the number of possible binary noun partitions is <EQN/> . </S>
<S ID='S-58' IA='OWN' AZ='OWN'> Thus for each binary noun partition we need <EQN/> bits . </S>
<S ID='S-59' IA='OWN' AZ='OWN'> Hence <EQN/> is calculated as  </S>
<IMAGE ID='I-4'/>
</P>
<P>
<S ID='S-60' IA='OWN' AZ='OWN'> <EQN/> is calculated by  </S>
<IMAGE ID='I-5'/>
</P>
<P>
<S ID='S-61' IA='OWN' AZ='OWN'> where | S | denotes the input data size , and <EQN/> is the number of ( free ) parameters in the model . </S>
<S ID='S-62' IA='OWN' AZ='OWN'> It is known that using <EQN/> <EQN/> bits to describe each of the parameters will ( approximately ) minimize the description length <REF TYPE='P'>Rissanen 1984</REF> . </S>
<S ID='S-63' IA='OWN' AZ='OWN'> Finally , <EQN/> is calculated by  </S>
<IMAGE ID='I-6'/>
</P>
<P>
<S ID='S-64' IA='OWN' AZ='OWN'> where <EQN/> denotes the observed frequency of the noun verb pair <EQN/> , and <EQN/> the estimated probability of <EQN/> , which is calculated as follows </S>
<IMAGE ID='I-7'/>
</P>
<P>
<S ID='S-65' IA='OWN' AZ='OWN'> where <EQN/> denotes the observed frequency of the noun verb pairs belonging to cluster <EQN/> . </S>
</P>
<P>
<S ID='S-66' IA='OWN' AZ='OWN'> With the description length of a model defined in the above manner , we wish to select a model having the minimum description length and output it as the result of clustering . </S>
<S ID='S-67' IA='OWN' AZ='OWN'> Since the model description length <EQN/> is the same for each model , in practice we only need to calculate and compare <EQN/> . </S>
</P>
<P>
<S ID='S-68' IA='OWN' AZ='OWN'> The description lengths for the data in Figure <CREF/> using the two models in Figure <CREF/> are shown in Table <CREF/> . </S>
<S ID='S-69' IA='OWN' AZ='OWN'> ( Table <CREF/> shows some values needed for the calculation of the description length for Model 1 . ) These calculations indicate that according to MDL , Model 1 should be selected over Model 2 . </S>
<IMAGE ID='I-8'/>
<IMAGE ID='I-9'/>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-4'> A Simulated Annealing-based Algorithm </HEADER>
<P>
<S ID='S-70' IA='OWN' AZ='OWN'> We could in principle calculate the description length for each model and select a model with the minimum description length , if computation time were of no concern . </S>
<S ID='S-71' IA='OWN' AZ='OWN'> However , since the number of probabilistic models under consideration is exponential , this is not feasible in practice . </S>
<S ID='S-72' IA='OWN' AZ='BAS' R='BAS'> We employ the ` simulated annealing technique ' to deal with this problem . </S>
<S ID='S-73' IA='OWN' AZ='OWN'> Figure <CREF/> shows our ( divisive ) clustering algorithm . </S>
<IMAGE ID='I-10'/>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-5'> Related Work </HEADER>
<P>
<S ID='S-74' IA='OWN' AZ='OWN'> Although there have been many methods of word clustering proposed to date , their objectives seem to vary . </S>
<S ID='S-75' IA='OWN' AZ='OWN'> In Table <CREF/> we exhibit a simple comparison between our work and related work . </S>
<S ID='S-76' IA='OWN' AZ='BAS' R='BAS'> Perhaps the method proposed by <REF TYPE='A'>Pereira et al. 1993</REF> is the most relevant in our context . </S>
<S ID='S-77' IA='OWN' AZ='OTH'> In <REF TYPE='A'>Pereira et al. 1993</REF> , they proposed a method of ` soft clustering , ' namely , each word can belong to a number of distinct classes with certain probabilities . </S>
<S ID='S-78' IA='OWN' AZ='OTH'> Soft clustering has several desirable properties . </S>
<S ID='S-79' IA='OWN' AZ='OTH'> For example , word sense ambiguities in input data can be treated in a unified manner . </S>
<S ID='S-80' IA='OWN' AZ='CTR' R='CTR'> Here , we restrict our attention on ` hard clustering ' ( i.e. , each word must belong to exactly one class ) , in part because we are interested in comparing the thesauri constructed by our method with existing hand-made thesauri . </S>
<S ID='S-81' IA='OWN' AZ='OWN'> ( Note that a hand made thesaurus is based on hard clustering . )</S>
<IMAGE ID='I-11'/>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-6'> Advantages of Our Method </HEADER>
<P>
<S ID='S-82' IA='OWN' AZ='TXT'> In this section , we elaborate on the merits of our method . </S>
</P>
<P>
<S ID='S-83' IA='OWN' AZ='BKG'> In statistical natural language processing , usually the number of parameters in a probabilistic model to be estimated is very large , and therefore such a model is difficult to estimate with a reasonable data size that is available in practice . </S>
<S ID='S-84' IA='OWN' AZ='BKG'> ( This problem is usually referred to as the ` data sparseness problem ' . )</S>
<S ID='S-85' IA='OWN' AZ='OTH'> We could smooth the estimated probabilities using an existing smoothing technique <REF TYPE='P'>Dagan et al. 1992</REF> , <REF TYPE='P'>Gale and Church 1990</REF> , then calculate some similarity measure using the smoothed probabilities , and then cluster words according to it . </S>
<S ID='S-86' IA='OWN' AZ='CTR' R='CTR'> There is no guarantee , however , that the employed smoothing method is in any way consistent with the clustering method used subsequently . </S>
<S ID='S-87' IA='OWN' AZ='OWN'> Our method based on MDL resolves this issue in a unified fashion . </S>
<S ID='S-88' IA='OWN' AZ='OWN'> By employing models that embody the assumption that words belonging to a same cluster occur in the same context with equal likelihood , our method achieves the smoothing effect as a side effect of the clustering process , where the domains of smoothing coincide with the clusters obtained by clustering . </S>
<S ID='S-89' IA='OWN' AZ='OWN'> Thus , the coarseness or fineness of clustering also determines the degree of smoothing . </S>
<S ID='S-90' IA='OWN' AZ='OWN'> All of these effects fall out naturally as a corollary of the imperative of ` best possible estimation , ' the original motivation behind the MDL principle . </S>
</P>
<P>
<S ID='S-91' IA='OWN' AZ='OTH'> In our simulated annealing algorithm , we could alternatively employ the Maximum Likelihood Estimator ( MLE ) as criterion for the best probabilistic model , instead of MDL . </S>
<S ID='S-92' IA='OWN' AZ='OTH'> MLE , as its name suggests , selects a model which maximizes the likelihood of the data , that is , <EQN/> . </S>
<S ID='S-93' IA='OWN' AZ='OTH'> This is equivalent to minimizing the ` data description length ' as defined in Section 3 , i.e. <EQN/> . </S>
<S ID='S-94' IA='OWN' AZ='OTH'> We can see easily that MDL generalizes MLE , in that it also takes into account the complexity of the model itself . </S>
<S ID='S-95' IA='OWN' AZ='CTR' R='CTR'> In the presence of models with varying complexity , MLE tends to overfit the data , and output a model that is too complex and tailored to fit the specifics of the input data . </S>
<S ID='S-96' IA='OWN' AZ='CTR'> If we employ MLE as criterion in our simulated annealing algorithm , it will result in selecting a very fine model with many small clusters , most of which will have probabilities estimated as zero . </S>
<S ID='S-97' IA='OWN' AZ='CTR'> Thus , in contrast to employing MDL , it will not have the effect of smoothing at all . </S>
</P>
<P>
<S ID='S-98' IA='OWN' AZ='CTR'> Purely as a method of estimation as well , the superiority of MDL over MLE is supported by convincing theoretical findings <REF TYPE='P'>Barron and Cover 1991</REF> , <REF TYPE='P'>Yamanishi 1992</REF> . </S>
<S ID='S-99' IA='OWN' AZ='OTH'> For instance , the speed of convergence of the models selected by MDL to the true model is known to be near optimal . </S>
<S ID='S-100' IA='OWN' AZ='CTR'> ( The models selected by MDL converge to the true model approximately at the rate of 1 / s where s is the number of parameters in the true model , whereas for MLE the rate is 1 / t , where t is the size of the domain , or in our context , the total number of elements of <EQN/> . ) </S>
<S ID='S-101' IA='OWN' AZ='CTR' R='CTR'> ` Consistency ' is another desirable property of MDL , which is not shared by MLE . </S>
<S ID='S-102' IA='OWN' AZ='OTH'> That is , the number of parameters in the models selected by MDL converge to that of the true model <REF TYPE='P'>Rissanen 1984</REF> . </S>
<S ID='S-103' IA='OWN' AZ='OWN'> Both of these properties of MDL are empirically verified in our present context , as will be shown in the next section . </S>
<S ID='S-104' IA='OWN' AZ='CTR' R='CTR'> In particular , we have compared the performance of employing an MDL-based simulated annealing against that of one based on MLE in word clustering . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-7'> Experimental Results </HEADER>
<P>
<S ID='S-105' IA='OWN' AZ='TXT'> We describe our experimental results in this section . </S>
</P>
<DIV DEPTH='2'>
<HEADER ID='H-8'> Experiment 1 : MDL v.s. MLE </HEADER>
<P>
<S ID='S-106' IA='OWN' AZ='BKG'> We compared the performance of employing MDL as a criterion in our simulated annealing algorithm , against that of employing MLE by simulation experiments . </S>
<S ID='S-107' IA='OWN' AZ='OWN'> We artificially constructed a true model of word co-occurrence , and then generated data according to its distribution . </S>
<S ID='S-108' IA='OWN' AZ='OWN'> We then used the data to estimate a model ( clustering words ) , and measured the KL distance between the true model and the estimated model . </S>
<S ID='S-109' IA='OWN' AZ='OWN'> ( The algorithm used for MLE was the same as that shown in Figure <CREF/> , except the ` data description length ' replaces the ( total ) description length ' in Step 2 . ) </S>
<S ID='S-110' IA='OWN' AZ='OWN'> Figure <CREF/> plots the relation between the number of obtained noun clusters ( leaf nodes in the obtained thesaurus tree ) versus the input data size , averaged over 10 trials . </S>
<S ID='S-111' IA='OWN' AZ='OWN'> ( The number of noun clusters in the true model is 4 . ) </S>
<S ID='S-112' IA='OWN' AZ='OWN'> Figure <CREF/>  plots the KL distance versus the data size , also averaged over the same 10 trials . </S>
<S ID='S-113' IA='OWN' AZ='CTR' R='CTR' HUMAN='RESULT;CLCO'> The results indicate that MDL converges to the true model faster than MLE . </S>
<S ID='S-114' IA='OWN' AZ='CTR'> Also , MLE tends to select a model overfitting the data , while MDL tends to select a model which is simple and yet fits the data reasonably well . </S>
<S ID='S-115' IA='OWN' AZ='OWN'> We conducted the same simulation experiment for some other models and found the same tendencies . </S>
<S ID='S-116' IA='OWN' AZ='OWN'> ( Figure <CREF/>  and Figure <CREF/>  show the analogous results when the number of noun clusters in the true model is 2 ) . </S>
<S ID='S-117' IA='OWN' AZ='CTR' R='CTR' HUMAN='CLCO'> We conclude that it is better to employ MDL than MLE in word clustering . </S>
<IMAGE ID='I-12'/>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-9'> Experiment 2 : Qualitative Evaluation </HEADER>
<IMAGE ID='I-13'/>
<P>
<S ID='S-118' IA='OWN' AZ='OWN'> We extracted roughly 180,000 case frames from the bracketed WSJ ( Wall Street Journal ) corpus of the Penn Tree Bank <REF TYPE='P'>Marcus et al. 1993</REF> as co-occurrence data . </S>
<S ID='S-119' IA='OWN' AZ='OWN'> We then constructed a number of thesauri based on these data , using our method . </S>
<S ID='S-120' IA='OWN' AZ='OWN'> Figure <CREF/> shows an example thesaurus for the 20 most frequently occurred nouns in the data , constructed based on their appearances as subject and object of roughly 2000 verbs . </S>
<S ID='S-121' IA='OWN' AZ='OWN'> The obtained thesaurus seems to agree with human intuition to some degree . </S>
<S ID='S-122' IA='OWN' AZ='OWN'> For example , ` million ' and ` billion ' are classified in one noun cluster , and ` stock ' and ` share ' are classified together . </S>
<S ID='S-123' IA='OWN' AZ='OWN'> Not all of the noun clusters , however , seem to be meaningful in the useful sense . </S>
<S ID='S-124' IA='OWN' AZ='OWN'> This is probably because the data size we had was not large enough . </S>
<S ID='S-125' IA='OWN' AZ='OWN'> This general tendency is also observed in another example thesaurus obtained by our method , shown in Figure <CREF/> . </S>
<S ID='S-126' IA='OWN' AZ='OWN'> Pragmatically speaking , however , whether the obtained thesaurus agrees with our intuition in itself is only of secondary concern , since the main purpose is to use the constructed thesaurus to help improve on a disambiguation task . </S>
</P>
</DIV>
<DIV DEPTH='2'>
<HEADER ID='H-10'> Experiment 3 : Disambiguation </HEADER>
<P>
<S ID='S-127' IA='OWN' AZ='OWN'> We also evaluated our method by using a constructed thesaurus in a pp-attachment disambiguation experiment . </S>
</P>
<P>
<S ID='S-128' IA='OWN' AZ='OWN'> We used as training data the same 180,000 case frames in Experiment 1 . </S>
<S ID='S-129' IA='OWN' AZ='OWN'> We also extracted as our test data 172 <EQN/> patterns from the data in the same corpus , which is not used in the training data . </S>
<S ID='S-130' IA='OWN' AZ='OWN'> For the 150 words that appear in the position of <EQN/> , we constructed a thesaurus based on the co-occurrences between heads and slot values of the frames in the training data . </S>
<S ID='S-131' IA='OWN' AZ='OWN'> This is because in our disambiguation test we only need a thesaurus consisting of these 150 words . </S>
<S ID='S-132' IA='OWN' AZ='BAS' R='BAS'> We then applied the learning method proposed in <REF TYPE='A' SELF="YES">Li and Abe 1995</REF> to learn case frame patterns with the constructed thesaurus as input using the same training data . </S>
<S ID='S-133' IA='OWN' AZ='BAS'> That is , we used it to learn the conditional distributions <EQN/> , <EQN/> , where <EQN/> and <EQN/> vary over the internal nodes in a certain ` cut ' in the thesaurus tree . </S>
<S ID='S-134' IA='OWN' AZ='OWN'> Table <CREF/> shows some example case frame patterns obtained by this method , and Figure <CREF/> shows the leaf nodes dominated by the internal nodes appearing in the case frame patterns of Table <CREF/> . </S>
<IMAGE ID='I-14'/>
<IMAGE ID='I-15'/>
<IMAGE ID='I-16'/>
</P>
<P>
<S ID='S-135' IA='OWN' AZ='OWN'> We then compare <EQN/> and <EQN/> , which are estimated based on the case frame patterns , to determine the attachment site of <EQN/> . </S>
<S ID='S-136' IA='OWN' AZ='OWN'> More specifically , if the former is larger than the latter , we attach it to verb , and if the latter is larger than the former , we attach it to <EQN/> , and otherwise ( including when both are 0 ) , we conclude that we cannot make a decision . </S>
<S ID='S-137' IA='OWN' AZ='OWN'> Table <CREF/> shows the results of our pp-attachment disambiguation experiment in terms of ` coverage ' and ` accuracy ' . </S>
<S ID='S-138' IA='OWN' AZ='OWN'> Here ` coverage ' refers to the proportion ( in percentage ) of the test patterns on which the disambiguation method could make a decision . </S>
<S ID='S-139' IA='OWN' AZ='OWN'> ` Base Line ' refers to the method of always attaching <EQN/> to <EQN/> . </S>
<S ID='S-140' IA='OWN' AZ='OWN'> ` Word-Based , ' ` MLE-Thesaurus , ' and ` MDL-Thesaurus ' respectively stand for using word-based estimates , using a thesaurus constructed by employing MLE , and using a thesaurus constructed by our method . </S>
<S ID='S-141' IA='OWN' AZ='CTR' R='CTR' HUMAN='SOLU;CLCO'> Note that the coverage of ` MDL-Thesaurus ' significantly outperformed that of ` Word-Based , ' while basically maintaining high accuracy ( though it drops somewhat ) , indicating that using an automatically constructed thesaurus can improve disambiguation results in terms of coverage . </S>
</P>
<P>
<S ID='S-142' IA='OWN' AZ='CTR' R='CTR' HUMAN='SOLU_local'> We also tested the method proposed in <REF TYPE='A' SELF="YES">Li and Abe 1995</REF> of learning case frames patterns using an existing thesaurus . </S>
<S ID='S-143' IA='OWN' AZ='BAS' R='BAS'> In particular , we used this method with WordNet <REF TYPE='P'>Miller et al. 1993</REF> and using the same training data , and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns . </S>
<S ID='S-144' IA='OWN' AZ='OWN'> We show the result of this experiment as ` WordNet ' in Table <CREF/> . </S>
<S ID='S-145' IA='OWN' AZ='OWN'> We can see that in terms of ` coverage , ' ` WordNet ' outperforms ` MDL-Thesaurus , ' but in terms of ` accuracy , ' ` MDL-Thesaurus ' outperforms ` WordNet ' . </S>
<S ID='S-146' IA='OWN' AZ='OWN'> These results can be interpreted as follows . </S>
<S ID='S-147' IA='OWN' AZ='OWN'> An automatically constructed thesaurus is more domain dependent and captures the domain dependent features better , and thus using it achieves high accuracy . </S>
<S ID='S-148' IA='OWN' AZ='OWN'> On the other hand , since training data we had available is insufficient , its coverage is smaller than that of a hand made thesaurus . </S>
<S ID='S-149' IA='OWN' AZ='OWN'> In practice , it makes sense to combine both types of thesauri . </S>
<S ID='S-150' IA='OWN' AZ='OWN'> More specifically , an automatically constructed thesaurus can be used within its coverage , and outside its coverage , a hand made thesaurus can be used . </S>
<S ID='S-151' IA='OWN' AZ='OWN'> Given the current state of the word clustering technique ( namely , it requires data size that is usually not available , and it tends to be computationally demanding ) , this strategy is practical . </S>
<S ID='S-152' IA='OWN' AZ='OWN'> We show the result of this combined method as ` MDL-Thesaurus + WordNet ' in Table <CREF/> . </S>
<S ID='S-153' IA='OWN' AZ='OWN'> Our experimental result shows that employing the combined method does increase the coverage of disambiguation . </S>
<S ID='S-154' IA='OWN' AZ='OWN'> We also tested ` MDL-Thesaurus + WordNet + LA + Default , ' which stands for using the learned thesaurus and WordNet first , then the lexical association value proposed by <REF TYPE='A'>Hindle and Rooth 1991</REF> , and finally the default ( i.e. always attaching <EQN/> to <EQN/> ) . </S>
<S ID='S-155' IA='OWN' AZ='CTR' R='CTR'> Our best disambiguation result obtained using this last combined method somewhat improves the accuracy reported in <REF TYPE='A' SELF="YES">Li and Abe 1995</REF> ( <EQN/> ) . </S>
<IMAGE ID='I-17'/>
</P>
</DIV>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-11'> Concluding Remarks </HEADER>
<P>
<S ID='S-156' ABSTRACTC='A-0' IA='OWN' AZ='AIM' R='AIM' HUMAN='PUPR_contrib'> We have proposed a method of clustering words based on large corpus data . </S>
<S ID='S-157' IA='OWN' AZ='TXT'> We conclude with the following remarks . </S>
</P>
<P>
<S ID='S-158' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_prop;CLCO'> Our method of hierarchical clustering of words based on the MDL principle is theoretically sound . </S>
<S ID='S-159' IA='OWN' AZ='CTR' R='CTR' HUMAN='CLCO'> Our experimental results show that it is better to employ MDL than MLE as estimation criterion in word clustering . </S>
</P>
<P>
<S ID='S-160' IA='OWN' AZ='OWN' R='OWN' HUMAN='SOLU_+,use;CLCO'> Using a thesaurus constructed by our method can improve pp-attachment disambiguation results . </S>
</P>
<P>
<S ID='S-161' IA='OWN' AZ='OWN' R='OWN' HUMAN='CLCO_recom'> At the current state of the art in statistical natural language processing , it is best to use a combination of an automatically constructed thesaurus and a hand made thesaurus for disambiguation purpose . </S>
<S ID='S-162' IA='OWN' AZ='OWN' R='OWN'> The disambiguation accuracy obtained this way was <EQN/> . </S>
</P>
<P>
<S ID='S-163' IA='OWN' AZ='OWN'> In the future , hopefully with larger training data size , we plan to construct larger thesauri as well as to test other clustering algorithms . </S>
</P>
</DIV>
<DIV DEPTH='1'>
<HEADER ID='H-12'> Acknowledgement </HEADER>
<P>
<S ID='S-164' AZ='OWN' IA='OWN'> We thank Mr. K. Nakamura , Mr. T. Fujita , and Dr. K. Kobayashi of NEC C &amp; C Res. Labs.  for their constant encouragement . </S>
<S ID='S-165' AZ='OWN' IA='OWN'> We thank Dr. K. Yamanishi of C &amp; C Res. Labs. for his comments . </S>
<S ID='S-166' AZ='OWN' IA='OWN'> We thank Ms. Y. Yamaguchi of NIS for her programming effort . </S>
</P>
</DIV>
</BODY>
<REFERENCES>
<REFERENCE>
Andrew R. <SURNAME>Barron</SURNAME> and Thomas M. <SURNAME>Cover</SURNAME>.
<DATE>1991</DATE>.
Minimum complexity density estimation.
IEEE Transaction on Information Theory, 37(4):<DATE>1034</DATE>-<DATE>1054</DATE>.
</REFERENCE>
<REFERENCE>
Peter F. <SURNAME>Brown</SURNAME>, Vincent J. <SURNAME>Della</SURNAME> <SURNAME>Pietra</SURNAME>, Peter V. <SURNAME>deSouza</SURNAME>, Jenifer C. <SURNAME>Lai</SURNAME>, and
  Robert L. Mercer.
<DATE>1992</DATE>.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):283-298.
</REFERENCE>
<REFERENCE>
Thomas M. <SURNAME>Cover</SURNAME> and Joy A. <SURNAME>Thomas</SURNAME>.
<DATE>1991</DATE>.
Elements of Information Theory.
John Wiley amp; Sons Inc.
</REFERENCE>
<REFERENCE>
Ido <SURNAME>Dagan</SURNAME>, Shaul <SURNAME>Marcus</SURNAME>, and Shaul <SURNAME>Makovitch</SURNAME>.
<DATE>1992</DATE>.
Contextual word similarity and estimation from sparse data.
Proceedings of the 30th ACL, pages 164-171.
</REFERENCE>
<REFERENCE>
Williams A. <SURNAME>Gale</SURNAME> and Kenth W. <SURNAME>Church</SURNAME>.
<DATE>1990</DATE>.
Poor estimates of context are worse than none.
Proceedings of the DARPA Speech and Natural Language Workshop,
  pages 283-287.
</REFERENCE>
<REFERENCE>
Donald <SURNAME>Hindle</SURNAME> and Mats <SURNAME>Rooth</SURNAME>.
<DATE>1991</DATE>.
Structural ambiguity and lexical relations.
Proceedings of the 29th ACL, pages 229-236.
</REFERENCE>
<REFERENCE>
Donald <SURNAME>Hindle</SURNAME>.
<DATE>1990</DATE>.
Noun classification from predicate-argument structures.
Proceedings of the 28th ACL, pages 268-275.
</REFERENCE>
<REFERENCE>
Hang <SURNAME>Li</SURNAME> and Naoki <SURNAME>Abe</SURNAME>.
<DATE>1995</DATE>.
Generalizing case frames using a thesaurus and the MDL principle.
Proceedings of Recent Advances in Natural Language Processing,
  pages 239-248.
</REFERENCE>
<REFERENCE>
Mitchell P. <SURNAME>Marcus</SURNAME>, Beatrice <SURNAME>Santorini</SURNAME>, and Mary <SURNAME>Ann</SURNAME> Marcinkiewicz.
<DATE>1993</DATE>.
Building a large annotated corpus of English: The penn treebank.
Computational Linguistics, 19(1):313-330.
</REFERENCE>
<REFERENCE>
George A. <SURNAME>Miller</SURNAME>, Richard <SURNAME>Beckwith</SURNAME>, Chirstiane <SURNAME>Fellbaum</SURNAME>, Derek <SURNAME>Gross</SURNAME>, and
  Katherine <SURNAME>Miller</SURNAME>.
<DATE>1993</DATE>.
Introduction to WordNet: An on-line lexical database.
Anonymous FTP: clarity.princeton.edu.
</REFERENCE>
<REFERENCE>
Fernando <SURNAME>Pereira</SURNAME>, Naftali <SURNAME>Tishby</SURNAME>, and Lillian <SURNAME>Lee</SURNAME>.
<DATE>1993</DATE>.
Distributional clustering of english words.
Proceedings of the 31st ACL, pages 183-190.
</REFERENCE>
<REFERENCE>
J. Ross <SURNAME>Quinlan</SURNAME> and Ronald L. <SURNAME>Rivest</SURNAME>.
<DATE>1989</DATE>.
Inferring decision trees using the minimum description length
  principle.
Information and Computation, 80:227-248.
</REFERENCE>
<REFERENCE>
Jorma <SURNAME>Rissanen</SURNAME>.
<DATE>1978</DATE>.
Modeling by shortest data description.
Automatic, 14:37-38.
</REFERENCE>
<REFERENCE>
Jorma <SURNAME>Rissanen</SURNAME>.
<DATE>1983</DATE>.
A universal prior for integers and estimation by minimum description
  length.
The Annals of Statistics, 11(2):416-431.
</REFERENCE>
<REFERENCE>
Jorma <SURNAME>Rissanen</SURNAME>.
<DATE>1984</DATE>.
Universal coding, information, predication and estimation.
IEEE Transaction on Information Theory, 30(4):629-636.
</REFERENCE>
<REFERENCE>
Jorma <SURNAME>Rissanen</SURNAME>.
<DATE>1986</DATE>.
Stochastic complexity and modeling.
The Annals of Statistics, 14(3):<DATE>1080</DATE>-<DATE>1100</DATE>.
</REFERENCE>
<REFERENCE>
Jorma <SURNAME>Rissanen</SURNAME>.
<DATE>1989</DATE>.
Stochastic Complexity in Statistical Inquiry.
World Scientific Publishing Co.
</REFERENCE>
<REFERENCE>
Andreas <SURNAME>Stolcke</SURNAME> and Stephen <SURNAME>Omohundro</SURNAME>.
<DATE>1994</DATE>.
Inducing probabilistic grammars by bayesian model merging.
Proceedings of ICGI'94.
</REFERENCE>
<REFERENCE>
Takenobu <SURNAME>Tokunaga</SURNAME>, Makoto <SURNAME>Iwayama</SURNAME>, and Hozumi <SURNAME>Tanaka</SURNAME>.
<DATE>1995</DATE>.
Automatic thesaurus construction based-on grammatical relations.
Proceedings of IJCAI'95.
</REFERENCE>
<REFERENCE>
Kenji <SURNAME>Yamanishi</SURNAME>.
<DATE>1992</DATE>.
A learning criterion for stochastic rules.
Machine Learning, 9:165-203.
</REFERENCE>
</REFERENCES>
</PAPER>
